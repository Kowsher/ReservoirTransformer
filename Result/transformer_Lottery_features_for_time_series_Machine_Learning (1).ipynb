{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qJzRm-WPvth7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/univ-length-over10.short-col-names.csv')\n",
        "\n",
        "#df=df.drop(columns = ['date'])\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "#df = imputer.fit_transform(df)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1= LabelEncoder()\n",
        "le2= LabelEncoder()\n",
        "le3= LabelEncoder()\n",
        "df['predWillFollowIncreaseCapacity']=le1.fit_transform(df['predWillFollowIncreaseCapacity'])\n",
        "df['Ft-ULen-Over10-didFollowIncreaseCapacity']=le2.fit_transform(df['Ft-ULen-Over10-didFollowIncreaseCapacity'])\n",
        "\n",
        "'''\n",
        "df=df.dropna()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "'''\n",
        "\n",
        "\n",
        "#df['icon']= le.fit_transform(df['icon'])\n",
        "\n",
        "y = df['Ft-ULen-Over10-didFollowIncreaseCapacity'].values\n",
        "df=df.drop(columns = ['Ft-ULen-Over10-didFollowIncreaseCapacity'])\n",
        "X = df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=X[1:]\n",
        "y=y[1:]\n",
        "\n",
        "for i in range(len(y)):\n",
        "    if y[i]==2:\n",
        "        y[i]=1"
      ],
      "metadata": {
        "id": "zWVnl7DBybTW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G8AhP-1JwJ79"
      },
      "outputs": [],
      "source": [
        "\n",
        "y=y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X=np.concatenate((X[1:], y[0:-1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wDfNyxfrwL-3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X= sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zjgmUHADwXIJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y[1:], test_size=0.2, shuffle=False, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MpxcgA6OwYmn"
      },
      "outputs": [],
      "source": [
        "x_train1 = x_train1.reshape((x_train1.shape[0], x_train1.shape[1], 1))\n",
        "x_test1 = x_test1.reshape((x_test1.shape[0], x_test1.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CgwvxfOowa9b"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dxqGaYRxwdER"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "74pYOQhZwfhX"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    #outputs = layers.Dense(1)(x)\n",
        "\n",
        "\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7KsIcGkwrHx",
        "outputId": "a83c608f-02d1-495d-fbfd-1d7a57a85fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 20, 1)       2           ['input_3[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 20, 1)       7169        ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 20, 1)        0           ['multi_head_attention_8[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (TFOpL  (None, 20, 1)       0           ['dropout_18[0][0]',             \n",
            " ambda)                                                           'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_16[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 20, 4)        8           ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 20, 4)        0           ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 20, 1)        5           ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_17 (TFOpL  (None, 20, 1)       0           ['conv1d_17[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_17[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 20, 1)       7169        ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 20, 1)        0           ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 20, 1)       0           ['dropout_20[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_18[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 20, 4)        8           ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 20, 4)        0           ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 20, 1)        5           ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 20, 1)       0           ['conv1d_19[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_19[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 20, 1)       7169        ['layer_normalization_20[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 20, 1)        0           ['multi_head_attention_10[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_20 (TFOpL  (None, 20, 1)       0           ['dropout_22[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_20[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 20, 4)        8           ['layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 20, 4)        0           ['conv1d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 20, 1)        5           ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 20, 1)       0           ['conv1d_21[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_21[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 20, 1)       7169        ['layer_normalization_22[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 20, 1)        0           ['multi_head_attention_11[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 20, 1)       0           ['dropout_24[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 20, 1)       2           ['tf.__operators__.add_22[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 20, 4)        8           ['layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 20, 4)        0           ['conv1d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 20, 1)        5           ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None, 20, 1)       0           ['conv1d_23[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 20)          0           ['tf.__operators__.add_23[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          2688        ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 2)            258         ['dropout_26[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,690\n",
            "Trainable params: 31,690\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "input_shape = x_train1.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"mean_absolute_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"mean_absolute_error\"],\n",
        ")\n",
        "'''\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUdAP2aoz-Nl",
        "outputId": "f8f83987-576b-48ee-862b-ec2a3df4ae99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 4s 185ms/step - loss: 0.7895 - accuracy: 0.5476 - val_loss: 0.5630 - val_accuracy: 0.7059\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.6131 - accuracy: 0.7294 - val_loss: 0.6260 - val_accuracy: 0.6891\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.5824 - accuracy: 0.7548 - val_loss: 0.5365 - val_accuracy: 0.7143\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.5336 - accuracy: 0.7780 - val_loss: 0.5234 - val_accuracy: 0.7311\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.5269 - accuracy: 0.7505 - val_loss: 0.5036 - val_accuracy: 0.7563\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.4823 - accuracy: 0.7822 - val_loss: 0.5041 - val_accuracy: 0.7395\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.4958 - accuracy: 0.7844 - val_loss: 0.4968 - val_accuracy: 0.7479\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.4532 - accuracy: 0.7970 - val_loss: 0.4874 - val_accuracy: 0.7563\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 0.4611 - accuracy: 0.8034 - val_loss: 0.4687 - val_accuracy: 0.7899\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.4467 - accuracy: 0.7970 - val_loss: 0.4679 - val_accuracy: 0.7479\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.4299 - accuracy: 0.7970 - val_loss: 0.4516 - val_accuracy: 0.7731\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.4319 - accuracy: 0.8013 - val_loss: 0.4472 - val_accuracy: 0.7731\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.4202 - accuracy: 0.8055 - val_loss: 0.4435 - val_accuracy: 0.7899\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.4165 - accuracy: 0.8161 - val_loss: 0.4581 - val_accuracy: 0.7731\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.4155 - accuracy: 0.8118 - val_loss: 0.4641 - val_accuracy: 0.7563\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.3715 - accuracy: 0.8478 - val_loss: 0.4316 - val_accuracy: 0.7815\n",
            "Epoch 17/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.3945 - accuracy: 0.8245 - val_loss: 0.4167 - val_accuracy: 0.7983\n",
            "Epoch 18/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3945 - accuracy: 0.8140 - val_loss: 0.4384 - val_accuracy: 0.7815\n",
            "Epoch 19/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3697 - accuracy: 0.8309 - val_loss: 0.4215 - val_accuracy: 0.7899\n",
            "Epoch 20/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3752 - accuracy: 0.8097 - val_loss: 0.4357 - val_accuracy: 0.7815\n",
            "Epoch 21/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3451 - accuracy: 0.8457 - val_loss: 0.4248 - val_accuracy: 0.7647\n",
            "Epoch 22/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3523 - accuracy: 0.8372 - val_loss: 0.4390 - val_accuracy: 0.7647\n",
            "Epoch 23/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.3517 - accuracy: 0.8330 - val_loss: 0.4105 - val_accuracy: 0.7815\n",
            "Epoch 24/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.3497 - accuracy: 0.8393 - val_loss: 0.3853 - val_accuracy: 0.8067\n",
            "Epoch 25/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3515 - accuracy: 0.8393 - val_loss: 0.4211 - val_accuracy: 0.7647\n",
            "Epoch 26/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3543 - accuracy: 0.8245 - val_loss: 0.4038 - val_accuracy: 0.7815\n",
            "Epoch 27/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3399 - accuracy: 0.8351 - val_loss: 0.3962 - val_accuracy: 0.7899\n",
            "Epoch 28/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.3226 - accuracy: 0.8584 - val_loss: 0.3794 - val_accuracy: 0.7983\n",
            "Epoch 29/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.3183 - accuracy: 0.8541 - val_loss: 0.3794 - val_accuracy: 0.7983\n",
            "Epoch 30/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3144 - accuracy: 0.8605 - val_loss: 0.3996 - val_accuracy: 0.7899\n",
            "Epoch 31/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3101 - accuracy: 0.8584 - val_loss: 0.3891 - val_accuracy: 0.7815\n",
            "Epoch 32/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2925 - accuracy: 0.8795 - val_loss: 0.3867 - val_accuracy: 0.7899\n",
            "Epoch 33/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.3033 - accuracy: 0.8584 - val_loss: 0.3715 - val_accuracy: 0.8067\n",
            "Epoch 34/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2859 - accuracy: 0.8689 - val_loss: 0.4086 - val_accuracy: 0.7815\n",
            "Epoch 35/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 0.2997 - accuracy: 0.8562 - val_loss: 0.3614 - val_accuracy: 0.8067\n",
            "Epoch 36/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2933 - accuracy: 0.8732 - val_loss: 0.4982 - val_accuracy: 0.7647\n",
            "Epoch 37/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3247 - accuracy: 0.8372 - val_loss: 0.3969 - val_accuracy: 0.7983\n",
            "Epoch 38/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3023 - accuracy: 0.8816 - val_loss: 0.3934 - val_accuracy: 0.7899\n",
            "Epoch 39/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3051 - accuracy: 0.8647 - val_loss: 0.4093 - val_accuracy: 0.8067\n",
            "Epoch 40/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.2802 - accuracy: 0.8837 - val_loss: 0.3606 - val_accuracy: 0.7983\n",
            "Epoch 41/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.2877 - accuracy: 0.8732 - val_loss: 0.3509 - val_accuracy: 0.8151\n",
            "Epoch 42/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2941 - accuracy: 0.8584 - val_loss: 0.3746 - val_accuracy: 0.8151\n",
            "Epoch 43/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2977 - accuracy: 0.8605 - val_loss: 0.3625 - val_accuracy: 0.8151\n",
            "Epoch 44/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2743 - accuracy: 0.8837 - val_loss: 0.3862 - val_accuracy: 0.8067\n",
            "Epoch 45/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2696 - accuracy: 0.8837 - val_loss: 0.3745 - val_accuracy: 0.8151\n",
            "Epoch 46/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2595 - accuracy: 0.8901 - val_loss: 0.3769 - val_accuracy: 0.8235\n",
            "Epoch 47/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2661 - accuracy: 0.8901 - val_loss: 0.3837 - val_accuracy: 0.8235\n",
            "Epoch 48/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2533 - accuracy: 0.8879 - val_loss: 0.3972 - val_accuracy: 0.8067\n",
            "Epoch 49/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2476 - accuracy: 0.8901 - val_loss: 0.3828 - val_accuracy: 0.7899\n",
            "Epoch 50/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2436 - accuracy: 0.8964 - val_loss: 0.4079 - val_accuracy: 0.7815\n",
            "Epoch 51/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2321 - accuracy: 0.8985 - val_loss: 0.4054 - val_accuracy: 0.7899\n",
            "Epoch 52/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2406 - accuracy: 0.8858 - val_loss: 0.4460 - val_accuracy: 0.7731\n",
            "Epoch 53/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2373 - accuracy: 0.8816 - val_loss: 0.4346 - val_accuracy: 0.7731\n",
            "Epoch 54/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2475 - accuracy: 0.8943 - val_loss: 0.4442 - val_accuracy: 0.7815\n",
            "Epoch 55/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2408 - accuracy: 0.8922 - val_loss: 0.4022 - val_accuracy: 0.7815\n",
            "Epoch 56/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2490 - accuracy: 0.9006 - val_loss: 0.3818 - val_accuracy: 0.7815\n",
            "Epoch 57/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2186 - accuracy: 0.9049 - val_loss: 0.4150 - val_accuracy: 0.7899\n",
            "Epoch 58/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2354 - accuracy: 0.9049 - val_loss: 0.3943 - val_accuracy: 0.7815\n",
            "Epoch 59/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2233 - accuracy: 0.9154 - val_loss: 0.4199 - val_accuracy: 0.7815\n",
            "Epoch 60/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2271 - accuracy: 0.9070 - val_loss: 0.4036 - val_accuracy: 0.7731\n",
            "Epoch 61/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2042 - accuracy: 0.9218 - val_loss: 0.4092 - val_accuracy: 0.7815\n",
            "Epoch 62/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2165 - accuracy: 0.8943 - val_loss: 0.3797 - val_accuracy: 0.7731\n",
            "Epoch 63/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2020 - accuracy: 0.9281 - val_loss: 0.4370 - val_accuracy: 0.7815\n",
            "Epoch 64/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2261 - accuracy: 0.8943 - val_loss: 0.4482 - val_accuracy: 0.7647\n",
            "Epoch 65/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2202 - accuracy: 0.9112 - val_loss: 0.4399 - val_accuracy: 0.7647\n",
            "Epoch 66/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1954 - accuracy: 0.9239 - val_loss: 0.3993 - val_accuracy: 0.7815\n",
            "Epoch 67/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2167 - accuracy: 0.9070 - val_loss: 0.4082 - val_accuracy: 0.7899\n",
            "Epoch 68/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.2212 - accuracy: 0.9070 - val_loss: 0.4619 - val_accuracy: 0.7731\n",
            "Epoch 69/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2148 - accuracy: 0.9027 - val_loss: 0.4299 - val_accuracy: 0.7731\n",
            "Epoch 70/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2178 - accuracy: 0.8985 - val_loss: 0.4555 - val_accuracy: 0.7899\n",
            "Epoch 71/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1981 - accuracy: 0.9133 - val_loss: 0.3969 - val_accuracy: 0.7815\n",
            "Epoch 72/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2220 - accuracy: 0.9027 - val_loss: 0.4480 - val_accuracy: 0.7983\n",
            "Epoch 73/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1917 - accuracy: 0.9218 - val_loss: 0.3892 - val_accuracy: 0.7815\n",
            "Epoch 74/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1986 - accuracy: 0.9091 - val_loss: 0.6277 - val_accuracy: 0.7395\n",
            "Epoch 75/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2254 - accuracy: 0.9027 - val_loss: 0.5335 - val_accuracy: 0.7563\n",
            "Epoch 76/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2359 - accuracy: 0.9006 - val_loss: 0.4370 - val_accuracy: 0.7815\n",
            "Epoch 77/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2509 - accuracy: 0.8901 - val_loss: 0.4160 - val_accuracy: 0.7815\n",
            "Epoch 78/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2352 - accuracy: 0.9027 - val_loss: 0.4711 - val_accuracy: 0.7815\n",
            "Epoch 79/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.2233 - accuracy: 0.9175 - val_loss: 0.4575 - val_accuracy: 0.7479\n",
            "Epoch 80/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2515 - accuracy: 0.8964 - val_loss: 0.3950 - val_accuracy: 0.7563\n",
            "Epoch 81/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2104 - accuracy: 0.9049 - val_loss: 0.5032 - val_accuracy: 0.7815\n",
            "Epoch 82/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2287 - accuracy: 0.9091 - val_loss: 0.4034 - val_accuracy: 0.7563\n",
            "Epoch 83/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2159 - accuracy: 0.9112 - val_loss: 0.5187 - val_accuracy: 0.7731\n",
            "Epoch 84/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.2125 - accuracy: 0.9006 - val_loss: 0.4961 - val_accuracy: 0.7815\n",
            "Epoch 85/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.2114 - accuracy: 0.9070 - val_loss: 0.5281 - val_accuracy: 0.7647\n",
            "Epoch 86/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1906 - accuracy: 0.9260 - val_loss: 0.4682 - val_accuracy: 0.7647\n",
            "Epoch 87/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1970 - accuracy: 0.9239 - val_loss: 0.4411 - val_accuracy: 0.7731\n",
            "Epoch 88/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2112 - accuracy: 0.9133 - val_loss: 0.6564 - val_accuracy: 0.7647\n",
            "Epoch 89/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1853 - accuracy: 0.9133 - val_loss: 0.5360 - val_accuracy: 0.7899\n",
            "Epoch 90/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1837 - accuracy: 0.9133 - val_loss: 0.4699 - val_accuracy: 0.8151\n",
            "Epoch 91/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1761 - accuracy: 0.9260 - val_loss: 0.5566 - val_accuracy: 0.7899\n",
            "Epoch 92/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1849 - accuracy: 0.9197 - val_loss: 0.5169 - val_accuracy: 0.7647\n",
            "Epoch 93/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2025 - accuracy: 0.9112 - val_loss: 0.4128 - val_accuracy: 0.7983\n",
            "Epoch 94/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1900 - accuracy: 0.9218 - val_loss: 0.5573 - val_accuracy: 0.7899\n",
            "Epoch 95/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2251 - accuracy: 0.9070 - val_loss: 0.6551 - val_accuracy: 0.7563\n",
            "Epoch 96/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1980 - accuracy: 0.9175 - val_loss: 0.5050 - val_accuracy: 0.7983\n",
            "Epoch 97/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2004 - accuracy: 0.9070 - val_loss: 0.6141 - val_accuracy: 0.7563\n",
            "Epoch 98/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1790 - accuracy: 0.9323 - val_loss: 0.5787 - val_accuracy: 0.7479\n",
            "Epoch 99/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.1855 - accuracy: 0.9197 - val_loss: 0.5528 - val_accuracy: 0.7395\n",
            "Epoch 100/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1724 - accuracy: 0.9154 - val_loss: 0.5145 - val_accuracy: 0.7731\n",
            "Epoch 101/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1907 - accuracy: 0.9133 - val_loss: 0.6232 - val_accuracy: 0.7647\n",
            "Epoch 102/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1563 - accuracy: 0.9450 - val_loss: 0.5450 - val_accuracy: 0.7815\n",
            "Epoch 103/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1854 - accuracy: 0.9175 - val_loss: 0.4967 - val_accuracy: 0.7647\n",
            "Epoch 104/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1867 - accuracy: 0.9260 - val_loss: 0.5402 - val_accuracy: 0.7563\n",
            "Epoch 105/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.1640 - accuracy: 0.9408 - val_loss: 0.5028 - val_accuracy: 0.8067\n",
            "Epoch 106/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1630 - accuracy: 0.9260 - val_loss: 0.6030 - val_accuracy: 0.7731\n",
            "Epoch 107/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1619 - accuracy: 0.9323 - val_loss: 0.5337 - val_accuracy: 0.7983\n",
            "Epoch 108/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1584 - accuracy: 0.9302 - val_loss: 0.5687 - val_accuracy: 0.8067\n",
            "Epoch 109/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.1600 - accuracy: 0.9260 - val_loss: 0.5837 - val_accuracy: 0.7815\n",
            "Epoch 110/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1618 - accuracy: 0.9408 - val_loss: 0.7223 - val_accuracy: 0.7647\n",
            "Epoch 111/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1617 - accuracy: 0.9302 - val_loss: 0.6720 - val_accuracy: 0.7815\n",
            "Epoch 112/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1393 - accuracy: 0.9577 - val_loss: 0.8134 - val_accuracy: 0.7731\n",
            "Epoch 113/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.1425 - accuracy: 0.9450 - val_loss: 0.6286 - val_accuracy: 0.7731\n",
            "Epoch 114/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1541 - accuracy: 0.9366 - val_loss: 0.7400 - val_accuracy: 0.7815\n",
            "Epoch 115/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1585 - accuracy: 0.9154 - val_loss: 0.6054 - val_accuracy: 0.7983\n",
            "Epoch 116/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1584 - accuracy: 0.9323 - val_loss: 0.6199 - val_accuracy: 0.7731\n",
            "Epoch 117/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1532 - accuracy: 0.9302 - val_loss: 0.7264 - val_accuracy: 0.7563\n",
            "Epoch 118/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1470 - accuracy: 0.9493 - val_loss: 0.5933 - val_accuracy: 0.7815\n",
            "Epoch 119/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1376 - accuracy: 0.9450 - val_loss: 0.8021 - val_accuracy: 0.7815\n",
            "Epoch 120/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1385 - accuracy: 0.9514 - val_loss: 0.6766 - val_accuracy: 0.7647\n",
            "Epoch 121/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1447 - accuracy: 0.9345 - val_loss: 0.7822 - val_accuracy: 0.7647\n",
            "Epoch 122/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1320 - accuracy: 0.9641 - val_loss: 0.6911 - val_accuracy: 0.7731\n",
            "Epoch 123/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.1243 - accuracy: 0.9514 - val_loss: 0.7787 - val_accuracy: 0.7647\n",
            "Epoch 124/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1305 - accuracy: 0.9471 - val_loss: 0.6698 - val_accuracy: 0.7731\n",
            "Epoch 125/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1572 - accuracy: 0.9323 - val_loss: 0.7412 - val_accuracy: 0.7815\n",
            "Epoch 126/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1197 - accuracy: 0.9577 - val_loss: 0.7848 - val_accuracy: 0.7647\n",
            "Epoch 127/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.1557 - accuracy: 0.9197 - val_loss: 0.8434 - val_accuracy: 0.7563\n",
            "Epoch 128/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1193 - accuracy: 0.9493 - val_loss: 0.7814 - val_accuracy: 0.7815\n",
            "Epoch 129/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1296 - accuracy: 0.9493 - val_loss: 0.7967 - val_accuracy: 0.7731\n",
            "Epoch 130/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1508 - accuracy: 0.9429 - val_loss: 0.6978 - val_accuracy: 0.7899\n",
            "Epoch 131/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.1407 - accuracy: 0.9408 - val_loss: 0.8240 - val_accuracy: 0.7815\n",
            "Epoch 132/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1331 - accuracy: 0.9535 - val_loss: 0.9430 - val_accuracy: 0.7479\n",
            "Epoch 133/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1533 - accuracy: 0.9366 - val_loss: 0.6832 - val_accuracy: 0.7899\n",
            "Epoch 134/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1541 - accuracy: 0.9366 - val_loss: 0.8429 - val_accuracy: 0.7647\n",
            "Epoch 135/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1417 - accuracy: 0.9471 - val_loss: 0.7258 - val_accuracy: 0.7731\n",
            "Epoch 136/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1619 - accuracy: 0.9345 - val_loss: 0.6874 - val_accuracy: 0.7815\n",
            "Epoch 137/1000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.1373 - accuracy: 0.9514 - val_loss: 1.0347 - val_accuracy: 0.7647\n",
            "Epoch 138/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1637 - accuracy: 0.9450 - val_loss: 0.7501 - val_accuracy: 0.7731\n",
            "Epoch 139/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1337 - accuracy: 0.9535 - val_loss: 1.1987 - val_accuracy: 0.7479\n",
            "Epoch 140/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.1549 - accuracy: 0.9408 - val_loss: 0.7053 - val_accuracy: 0.7815\n",
            "Epoch 141/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.1553 - accuracy: 0.9345 - val_loss: 0.9675 - val_accuracy: 0.7479\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)]\n",
        "\n",
        "history=model.fit(\n",
        "    x_train1,\n",
        "    y_train1,\n",
        "    validation_data=(x_test1, y_test1),\n",
        "    epochs=1000,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueh9al7Gf26I"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIz0Rwc1ftLk",
        "outputId": "a5942897-c3ca-4cf8-90a0-82a533121a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.8151260504201681\n",
            "f1_score 0.7608695652173912\n",
            "jaccard_score 0.6140350877192983\n",
            "roc_auc_score 0.8078078078078078\n",
            "precision_score 0.7446808510638298\n",
            "recall_score 0.7777777777777778\n"
          ]
        }
      ],
      "source": [
        "#testing prediction\n",
        "ypred1=model.predict(x_test1)\n",
        "ypred=[]\n",
        "#testing prediction\n",
        "for i in ypred1:\n",
        "    if i[0]>i[1]:\n",
        "        ypred.append(0)\n",
        "    else:\n",
        "        ypred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(ypred, y_test1))\n",
        "print('roc_auc_score', roc_auc_score(ypred, y_test1))\n",
        "print('precision_score', precision_score(ypred, y_test1))\n",
        "\n",
        "print('recall_score', recall_score(ypred, y_test1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BqSdi172JdD",
        "outputId": "41ecda9e-ce7b-4305-8bce-17049d23afda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.8964059196617337\n",
            "f1_score 0.8545994065281898\n",
            "jaccard_score 0.7461139896373057\n",
            "roc_auc_score 0.9153202419421659\n",
            "precision_score 0.7659574468085106\n",
            "recall_score 0.9664429530201343\n"
          ]
        }
      ],
      "source": [
        "#ttraining prediction\n",
        "xpred1=model.predict(x_train1)\n",
        "xpred=[]\n",
        "\n",
        "for i in xpred1:\n",
        "    if i[0]>i[1]:\n",
        "        xpred.append(0)\n",
        "    else:\n",
        "        xpred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(xpred, y_train1))\n",
        "print('roc_auc_score', roc_auc_score(xpred, y_train1))\n",
        "print('precision_score', precision_score(xpred, y_train1))\n",
        "\n",
        "print('recall_score', recall_score(xpred, y_train1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XMSQfLXdi6da"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_Lottery_features_for_time_series_Machine_Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}