{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M2icok5AYzvS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/IOT-temp 2.csv')\n",
        "\n",
        "df=df.drop(columns = ['id', 'room_id/id', 'noted_date'])\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "#df = imputer.fit_transform(df)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1= LabelEncoder()\n",
        "\n",
        "\n",
        "df['out/in']= le1.fit_transform(df['out/in'])\n",
        "\n",
        "'''\n",
        "df=df.dropna()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "'''\n",
        "\n",
        "\n",
        "#df['icon']= le.fit_transform(df['icon'])\n",
        "\n",
        "y = df['out/in'].values\n",
        "df=df.drop(columns = ['out/in'])\n",
        "X = df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guBeZhMnFYBm",
        "outputId": "3c74bfd3-2017-42c2-eef8-201318557677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgyrLr_T-2Ok",
        "outputId": "834b274e-42df-4db7-eee3-012996241fb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(594, 19)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_82uIQCFdziZ",
        "outputId": "a63ee74f-59c9-43f4-a593-1b7edceea251"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 20345, 1: 77261})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from collections import Counter\n",
        "Counter(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Rn2VoK1owI5"
      },
      "outputs": [],
      "source": [
        "\n",
        "y=y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X=np.concatenate((X[1:], y[0:-1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxWNccXgu12l",
        "outputId": "8232c071-622b-46d2-b72c-3e5d9767575a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=ce72735f57a4e2d8e301cf849c83355d2600a165783e8958016f3bb1a456bb57\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8iIZ1105tFnf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X= sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl0SFJ5WtbBP",
        "outputId": "ac45960f-bb2f-45f8-bc03-5748818aaff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrcn\n",
            "  Downloading PyRCN-0.0.16.post1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyrcn\n",
            "Successfully installed pyrcn-0.0.16.post1\n"
          ]
        }
      ],
      "source": [
        "pip install pyrcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gEPWGptbuTYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y[1:], test_size=0.2, shuffle=False, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDAxxhLupYK6",
        "outputId": "3c343778-2705-4f33-89b7-e9047657317d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(78084, 2)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1rOeQ0-ptl_G"
      },
      "outputs": [],
      "source": [
        "from pyrcn.base.blocks import InputToNode\n",
        "from sklearn . datasets import make_blobs\n",
        "# Generate a toy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tShY5ktnrN",
        "outputId": "e3ba7439-2c3b-45f9-b319-145c76a22173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=20 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "input_to_node = InputToNode (20, input_activation='relu',input_scaling =1.0 )\n",
        "\n",
        "\n",
        "x_train= input_to_node.fit_transform (x_train1)\n",
        "x_test= input_to_node.transform (x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDw3vyTRuB-o",
        "outputId": "5c18876b-3638-4af7-c3e0-dcdeda2766d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=20 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from pyrcn.base.blocks import NodeToNode\n",
        "node_to_node = NodeToNode (20, reservoir_activation='relu', spectral_radius =0.99 , leakage =0.5,bidirectional = False )\n",
        "x_train=node_to_node . fit_transform(x_train)\n",
        "x_test= node_to_node.transform (x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKkfOFKFh0Hu",
        "outputId": "8ae55759-9c47-4796-bf0b-31fe1caa4c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.        , 22.00681312, 10.25975643,  5.15342963,  0.        ,\n",
              "        0.        ,  0.        , 16.85608116])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0eGix0lh2Gz",
        "outputId": "d93f22f3-9b6b-4639-f206-1487a9cf9446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.        , 32.89883769,  8.44760075,  0.        ,  6.02993687,\n",
              "        0.        ,  0.        ,  8.98919012])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2K_But1if0U",
        "outputId": "74d565dc-7ad6-4770-eb3f-b0a641675b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        9.39921364e-01, 0.00000000e+00, 3.31548435e-02, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 2.26567590e-01, 0.00000000e+00,\n",
              "        1.08160130e+00, 0.00000000e+00, 6.63096869e-03, 6.24059763e-02],\n",
              "       [0.00000000e+00, 0.00000000e+00, 2.49592404e-01, 0.00000000e+00,\n",
              "        1.30757630e+00, 0.00000000e+00, 1.32619374e-03, 1.24811953e-02],\n",
              "       [0.00000000e+00, 0.00000000e+00, 2.96601895e-01, 0.00000000e+00,\n",
              "        1.28755986e+00, 0.00000000e+00, 2.65238748e-04, 2.49623905e-03],\n",
              "       [0.00000000e+00, 0.00000000e+00, 2.87851835e-01, 0.00000000e+00,\n",
              "        1.40041279e+00, 0.00000000e+00, 5.30477495e-05, 4.99247810e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPI4KAxEjjEj",
        "outputId": "096d8873-5ca0-4e72-bb30-43f9421d9393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.59295759e-307, 0.00000000e+000, 3.17049322e-001,\n",
              "        0.00000000e+000, 1.23868465e+000, 2.29709269e-005,\n",
              "        2.85245730e-004, 8.14299262e-005],\n",
              "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
              "        0.00000000e+000, 9.39921364e-001, 0.00000000e+000,\n",
              "        3.31548435e-002, 0.00000000e+000],\n",
              "       [0.00000000e+000, 0.00000000e+000, 2.26567590e-001,\n",
              "        0.00000000e+000, 1.08160130e+000, 0.00000000e+000,\n",
              "        6.63096869e-003, 6.24059763e-002],\n",
              "       [0.00000000e+000, 0.00000000e+000, 2.49592404e-001,\n",
              "        0.00000000e+000, 1.30757630e+000, 0.00000000e+000,\n",
              "        1.32619374e-003, 1.24811953e-002],\n",
              "       [0.00000000e+000, 0.00000000e+000, 2.96601895e-001,\n",
              "        0.00000000e+000, 1.28755986e+000, 0.00000000e+000,\n",
              "        2.65238748e-004, 2.49623905e-003],\n",
              "       [0.00000000e+000, 0.00000000e+000, 2.87851835e-001,\n",
              "        0.00000000e+000, 1.40041279e+000, 0.00000000e+000,\n",
              "        5.30477495e-005, 4.99247810e-004],\n",
              "       [0.00000000e+000, 0.00000000e+000, 3.15017468e-001,\n",
              "        0.00000000e+000, 1.32984788e+000, 0.00000000e+000,\n",
              "        1.06095499e-005, 9.98495620e-005],\n",
              "       [0.00000000e+000, 0.00000000e+000, 2.96502227e-001,\n",
              "        0.00000000e+000, 1.41960632e+000, 0.00000000e+000,\n",
              "        2.12190998e-006, 1.99699124e-005],\n",
              "       [0.00000000e+000, 0.00000000e+000, 3.18989745e-001,\n",
              "        0.00000000e+000, 1.33858615e+000, 0.00000000e+000,\n",
              "        4.24381996e-007, 3.99398248e-006],\n",
              "       [0.00000000e+000, 0.00000000e+000, 2.98321503e-001,\n",
              "        0.00000000e+000, 1.42357952e+000, 0.00000000e+000,\n",
              "        8.48763993e-008, 7.98796496e-007]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=x_test[0]\n",
        "for i in range(1, len(x_test)):\n",
        "    b=x_test[i].copy()\n",
        "\n",
        "    x_test[i]=a\n",
        "    a=b.copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "UhIEjXnuihxV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2eH1uhW2hqrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0]=x_train[-1]\n",
        "x_train1=x_train1[1:]\n",
        "y_train1=y_train1[1:]\n",
        "\n",
        "x_train=x_train[:-1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QucN9X7rhSqW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dcmBtI2ThoGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b-dqEbXtuCpY"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "x_train1 = x_train1.reshape((x_train1.shape[0], x_train1.shape[1], 1))\n",
        "x_test1 = x_test1.reshape((x_test1.shape[0], x_test1.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qi0MhVmsuswU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NX3w9IqBuuZ-"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HsU6bO-PuvqD"
      },
      "outputs": [],
      "source": [
        "from keras_self_attention import SeqSelfAttention\n",
        "def build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputsy = keras.Input(shape=input_shapey)\n",
        "    inputsz = keras.Input(shape=input_shapez)\n",
        "    y = inputsy\n",
        "    z= inputsz\n",
        "\n",
        "    #z=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(z)\n",
        "\n",
        "    #y=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(y)\n",
        "\n",
        "    z=SeqSelfAttention()(z)\n",
        "    z=layers.Flatten()(z)\n",
        "    \n",
        "    z=layers.Dense(10, activation=\"tanh\")(z)\n",
        "    z=layers.Dense(5, activation=\"tanh\")(z)\n",
        "    z= layers.Reshape((-1,1))(z)\n",
        "\n",
        "\n",
        "\n",
        "    #y=SeqSelfAttention()(y)\n",
        "\n",
        "\n",
        "\n",
        "    x=layers.Concatenate(axis=1)([y, z])\n",
        "    #x=layers.Add()([inputsy, inputsz])\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    #outputs = layers.Dense(1)(x)\n",
        "    return keras.Model([inputsy,inputsz], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Hu3CH4vZzu",
        "outputId": "ace13807-a2dc-46e5-c4d7-2a16700be2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " seq_self_attention_2 (SeqSelfA  (None, 20, 1)       129         ['input_6[0][0]']                \n",
            " ttention)                                                                                        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 20)           0           ['seq_self_attention_2[0][0]']   \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 10)           210         ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 5)            55          ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 2, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 5, 1)         0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 7, 1)         0           ['input_5[0][0]',                \n",
            "                                                                  'reshape_2[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 7, 1)        2           ['concatenate_2[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 7, 1)        71          ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 7, 1)        0           ['dropout_14[0][0]',             \n",
            " ambda)                                                           'concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 7, 1)         0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 7, 1)         2           ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 7, 1)        0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 7, 1)        71          ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 7, 1)        0           ['dropout_16[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 7, 1)         0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 7, 1)         2           ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 7, 1)        0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_15[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 7, 1)        71          ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_8[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (TFOpL  (None, 7, 1)        0           ['dropout_18[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_15[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_16[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 7, 1)         0           ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 7, 1)         2           ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_17 (TFOpL  (None, 7, 1)        0           ['conv1d_17[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_17[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 7, 1)        71          ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 7, 1)        0           ['dropout_20[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_18[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 7, 1)         0           ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 7, 1)         2           ['dropout_21[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 7, 1)        0           ['conv1d_19[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_19[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 7, 1)        71          ['layer_normalization_20[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_10[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_20 (TFOpL  (None, 7, 1)        0           ['dropout_22[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_19[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_20[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 7, 1)         0           ['conv1d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 7, 1)         2           ['dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 7, 1)        0           ['conv1d_21[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_21[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 7, 1)        71          ['layer_normalization_22[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_11[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 7, 1)        0           ['dropout_24[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_22[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 7, 1)         0           ['conv1d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 7, 1)         2           ['dropout_25[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None, 7, 1)        0           ['conv1d_23[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_23[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (Multi  (None, 7, 1)        71          ['layer_normalization_24[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_12[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_24 (TFOpL  (None, 7, 1)        0           ['dropout_26[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_23[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_24[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 7, 1)         0           ['conv1d_24[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 7, 1)         2           ['dropout_27[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_25 (TFOpL  (None, 7, 1)        0           ['conv1d_25[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_25[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (Multi  (None, 7, 1)        71          ['layer_normalization_26[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_13[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_26 (TFOpL  (None, 7, 1)        0           ['dropout_28[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_26[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 7, 1)         0           ['conv1d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 7, 1)         2           ['dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_27 (TFOpL  (None, 7, 1)        0           ['conv1d_27[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_27[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (Multi  (None, 7, 1)        71          ['layer_normalization_28[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_14[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_28 (TFOpL  (None, 7, 1)        0           ['dropout_30[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_27[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_28[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 7, 1)         0           ['conv1d_28[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 7, 1)         2           ['dropout_31[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_29 (TFOpL  (None, 7, 1)        0           ['conv1d_29[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_29[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (Multi  (None, 7, 1)        71          ['layer_normalization_30[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 7, 1)         0           ['multi_head_attention_15[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_30 (TFOpL  (None, 7, 1)        0           ['dropout_32[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 7, 1)        2           ['tf.__operators__.add_30[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 7, 1)         2           ['layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)           (None, 7, 1)         0           ['conv1d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_31 (Conv1D)             (None, 7, 1)         2           ['dropout_33[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_31 (TFOpL  (None, 7, 1)        0           ['conv1d_31[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 7)           0           ['tf.__operators__.add_31[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 64)           512         ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 64)           0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 2)            130         ['dropout_34[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,826\n",
            "Trainable params: 1,826\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shapey = x_train1.shape[1:]\n",
        "input_shapez = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size=5,\n",
        "    num_heads=2,\n",
        "    ff_dim=1,\n",
        "    num_transformer_blocks=10,\n",
        "    mlp_units=[64],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"mean_absolute_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"mean_absolute_error\"],\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jta9P_Q3vkrc",
        "outputId": "427b0810-2482-4def-9edb-fd7610daa9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "611/611 [==============================] - 23s 31ms/step - loss: 0.2080 - sparse_categorical_accuracy: 0.9132 - val_loss: 0.5023 - val_sparse_categorical_accuracy: 0.7615\n",
            "Epoch 2/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1446 - sparse_categorical_accuracy: 0.9361 - val_loss: 0.4753 - val_sparse_categorical_accuracy: 0.7580\n",
            "Epoch 3/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1362 - sparse_categorical_accuracy: 0.9374 - val_loss: 0.4597 - val_sparse_categorical_accuracy: 0.7555\n",
            "Epoch 4/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1325 - sparse_categorical_accuracy: 0.9381 - val_loss: 0.4605 - val_sparse_categorical_accuracy: 0.7666\n",
            "Epoch 5/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1297 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.4792 - val_sparse_categorical_accuracy: 0.7530\n",
            "Epoch 6/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1292 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.4696 - val_sparse_categorical_accuracy: 0.7586\n",
            "Epoch 7/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1281 - sparse_categorical_accuracy: 0.9389 - val_loss: 0.4657 - val_sparse_categorical_accuracy: 0.7563\n",
            "Epoch 8/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1266 - sparse_categorical_accuracy: 0.9394 - val_loss: 0.4641 - val_sparse_categorical_accuracy: 0.7639\n",
            "Epoch 9/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1254 - sparse_categorical_accuracy: 0.9398 - val_loss: 0.4718 - val_sparse_categorical_accuracy: 0.7423\n",
            "Epoch 10/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1257 - sparse_categorical_accuracy: 0.9394 - val_loss: 0.4611 - val_sparse_categorical_accuracy: 0.7583\n",
            "Epoch 11/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1238 - sparse_categorical_accuracy: 0.9404 - val_loss: 0.4599 - val_sparse_categorical_accuracy: 0.7600\n",
            "Epoch 12/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.4553 - val_sparse_categorical_accuracy: 0.7624\n",
            "Epoch 13/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9407 - val_loss: 0.4563 - val_sparse_categorical_accuracy: 0.7601\n",
            "Epoch 14/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1210 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.4541 - val_sparse_categorical_accuracy: 0.7659\n",
            "Epoch 15/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1201 - sparse_categorical_accuracy: 0.9418 - val_loss: 0.4594 - val_sparse_categorical_accuracy: 0.7617\n",
            "Epoch 16/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1194 - sparse_categorical_accuracy: 0.9421 - val_loss: 0.4616 - val_sparse_categorical_accuracy: 0.7629\n",
            "Epoch 17/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1186 - sparse_categorical_accuracy: 0.9423 - val_loss: 0.4541 - val_sparse_categorical_accuracy: 0.7628\n",
            "Epoch 18/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1175 - sparse_categorical_accuracy: 0.9426 - val_loss: 0.4498 - val_sparse_categorical_accuracy: 0.7599\n",
            "Epoch 19/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1185 - sparse_categorical_accuracy: 0.9423 - val_loss: 0.4573 - val_sparse_categorical_accuracy: 0.7534\n",
            "Epoch 20/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1159 - sparse_categorical_accuracy: 0.9436 - val_loss: 0.4481 - val_sparse_categorical_accuracy: 0.7608\n",
            "Epoch 21/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1145 - sparse_categorical_accuracy: 0.9445 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.7631\n",
            "Epoch 22/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1139 - sparse_categorical_accuracy: 0.9455 - val_loss: 0.4466 - val_sparse_categorical_accuracy: 0.7615\n",
            "Epoch 23/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1132 - sparse_categorical_accuracy: 0.9454 - val_loss: 0.4461 - val_sparse_categorical_accuracy: 0.7668\n",
            "Epoch 24/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1128 - sparse_categorical_accuracy: 0.9466 - val_loss: 0.4706 - val_sparse_categorical_accuracy: 0.7594\n",
            "Epoch 25/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1122 - sparse_categorical_accuracy: 0.9474 - val_loss: 0.4489 - val_sparse_categorical_accuracy: 0.7609\n",
            "Epoch 26/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1100 - sparse_categorical_accuracy: 0.9478 - val_loss: 0.4488 - val_sparse_categorical_accuracy: 0.7648\n",
            "Epoch 27/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1091 - sparse_categorical_accuracy: 0.9486 - val_loss: 0.4581 - val_sparse_categorical_accuracy: 0.7540\n",
            "Epoch 28/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9463 - val_loss: 0.4441 - val_sparse_categorical_accuracy: 0.7692\n",
            "Epoch 29/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1081 - sparse_categorical_accuracy: 0.9497 - val_loss: 0.4951 - val_sparse_categorical_accuracy: 0.7511\n",
            "Epoch 30/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1118 - sparse_categorical_accuracy: 0.9481 - val_loss: 0.4474 - val_sparse_categorical_accuracy: 0.7694\n",
            "Epoch 31/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1066 - sparse_categorical_accuracy: 0.9498 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.7672\n",
            "Epoch 32/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1061 - sparse_categorical_accuracy: 0.9507 - val_loss: 0.4389 - val_sparse_categorical_accuracy: 0.7756\n",
            "Epoch 33/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1058 - sparse_categorical_accuracy: 0.9507 - val_loss: 0.4427 - val_sparse_categorical_accuracy: 0.7720\n",
            "Epoch 34/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1057 - sparse_categorical_accuracy: 0.9507 - val_loss: 0.4480 - val_sparse_categorical_accuracy: 0.7699\n",
            "Epoch 35/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1057 - sparse_categorical_accuracy: 0.9508 - val_loss: 0.4490 - val_sparse_categorical_accuracy: 0.7740\n",
            "Epoch 36/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1047 - sparse_categorical_accuracy: 0.9518 - val_loss: 0.4345 - val_sparse_categorical_accuracy: 0.7839\n",
            "Epoch 37/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1042 - sparse_categorical_accuracy: 0.9521 - val_loss: 0.4447 - val_sparse_categorical_accuracy: 0.7726\n",
            "Epoch 38/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1042 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.7743\n",
            "Epoch 39/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1026 - sparse_categorical_accuracy: 0.9525 - val_loss: 0.4525 - val_sparse_categorical_accuracy: 0.7750\n",
            "Epoch 40/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1022 - sparse_categorical_accuracy: 0.9523 - val_loss: 0.4409 - val_sparse_categorical_accuracy: 0.7795\n",
            "Epoch 41/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1038 - sparse_categorical_accuracy: 0.9522 - val_loss: 0.4461 - val_sparse_categorical_accuracy: 0.7835\n",
            "Epoch 42/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9524 - val_loss: 0.4416 - val_sparse_categorical_accuracy: 0.7904\n",
            "Epoch 43/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1026 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.4507 - val_sparse_categorical_accuracy: 0.7826\n",
            "Epoch 44/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.4383 - val_sparse_categorical_accuracy: 0.7874\n",
            "Epoch 45/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9532 - val_loss: 0.4324 - val_sparse_categorical_accuracy: 0.7884\n",
            "Epoch 46/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9536 - val_loss: 0.4283 - val_sparse_categorical_accuracy: 0.7921\n",
            "Epoch 47/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1003 - sparse_categorical_accuracy: 0.9539 - val_loss: 0.4440 - val_sparse_categorical_accuracy: 0.7816\n",
            "Epoch 48/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1000 - sparse_categorical_accuracy: 0.9540 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.7831\n",
            "Epoch 49/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0996 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.4575 - val_sparse_categorical_accuracy: 0.7743\n",
            "Epoch 50/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1001 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.4723 - val_sparse_categorical_accuracy: 0.7722\n",
            "Epoch 51/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1007 - sparse_categorical_accuracy: 0.9532 - val_loss: 0.4390 - val_sparse_categorical_accuracy: 0.7832\n",
            "Epoch 52/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0992 - sparse_categorical_accuracy: 0.9543 - val_loss: 0.4279 - val_sparse_categorical_accuracy: 0.7866\n",
            "Epoch 53/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0986 - sparse_categorical_accuracy: 0.9549 - val_loss: 0.4447 - val_sparse_categorical_accuracy: 0.7845\n",
            "Epoch 54/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0979 - sparse_categorical_accuracy: 0.9553 - val_loss: 0.5058 - val_sparse_categorical_accuracy: 0.7452\n",
            "Epoch 55/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.1011 - sparse_categorical_accuracy: 0.9539 - val_loss: 0.4320 - val_sparse_categorical_accuracy: 0.7885\n",
            "Epoch 56/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0989 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.4861 - val_sparse_categorical_accuracy: 0.7682\n",
            "Epoch 57/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0977 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.4328 - val_sparse_categorical_accuracy: 0.7883\n",
            "Epoch 58/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0985 - sparse_categorical_accuracy: 0.9546 - val_loss: 0.4286 - val_sparse_categorical_accuracy: 0.7878\n",
            "Epoch 59/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0960 - sparse_categorical_accuracy: 0.9547 - val_loss: 0.4323 - val_sparse_categorical_accuracy: 0.7920\n",
            "Epoch 60/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0970 - sparse_categorical_accuracy: 0.9552 - val_loss: 0.4260 - val_sparse_categorical_accuracy: 0.7959\n",
            "Epoch 61/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0996 - sparse_categorical_accuracy: 0.9538 - val_loss: 0.4567 - val_sparse_categorical_accuracy: 0.7763\n",
            "Epoch 62/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0974 - sparse_categorical_accuracy: 0.9548 - val_loss: 0.4496 - val_sparse_categorical_accuracy: 0.7800\n",
            "Epoch 63/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.4466 - val_sparse_categorical_accuracy: 0.7815\n",
            "Epoch 64/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0977 - sparse_categorical_accuracy: 0.9551 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.7890\n",
            "Epoch 65/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0973 - sparse_categorical_accuracy: 0.9553 - val_loss: 0.4387 - val_sparse_categorical_accuracy: 0.7815\n",
            "Epoch 66/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0966 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.4377 - val_sparse_categorical_accuracy: 0.7929\n",
            "Epoch 67/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0966 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.4652 - val_sparse_categorical_accuracy: 0.7825\n",
            "Epoch 68/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0965 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.4453 - val_sparse_categorical_accuracy: 0.7921\n",
            "Epoch 69/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0961 - sparse_categorical_accuracy: 0.9554 - val_loss: 0.4446 - val_sparse_categorical_accuracy: 0.7895\n",
            "Epoch 70/100\n",
            "611/611 [==============================] - 18s 30ms/step - loss: 0.0957 - sparse_categorical_accuracy: 0.9552 - val_loss: 0.4348 - val_sparse_categorical_accuracy: 0.7910\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "history=model.fit(\n",
        "    [x_train1,x_train],\n",
        "    y_train1,\n",
        "    validation_data=([x_test1, x_test], y_test1),\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42doJdzh7mKo",
        "outputId": "47ae7ff6-847e-4b8d-94cb-fcf95a18d8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8023666820347318\n",
            "f1_score 0.8432599333712523\n",
            "jaccard_score 0.7289969092441697\n",
            "roc_auc_score 0.7947379758707105\n",
            "precision_score 0.8672906568611065\n",
            "recall_score 0.8205249841872233\n"
          ]
        }
      ],
      "source": [
        "#testing prediction\n",
        "\n",
        "#testing prediction\n",
        "ypred1=model.predict([x_test1, x_test])\n",
        "\n",
        "ypred=[]\n",
        "#testing prediction\n",
        "for i in ypred1:\n",
        "    if i[0]>i[1]:\n",
        "        ypred.append(0)\n",
        "    else:\n",
        "        ypred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print(accuracy_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(ypred, y_test1))\n",
        "print('roc_auc_score', roc_auc_score(ypred, y_test1))\n",
        "print('precision_score', precision_score(ypred, y_test1))\n",
        "\n",
        "print('recall_score', recall_score(ypred, y_test1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9Rqyi5DuRJ",
        "outputId": "40609f5c-d984-4959-aaf7-75b07aaa8c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9593253332991817\n",
            "f1_score 0.9758122248792895\n",
            "jaccard_score 0.9527669130441249\n",
            "roc_auc_score 0.9343156984228156\n",
            "precision_score 0.9811624167240983\n",
            "recall_score 0.9705200648376786\n"
          ]
        }
      ],
      "source": [
        "#training prediction\n",
        "xpred1=model.predict([x_train1, x_train])\n",
        "xpred=[]\n",
        "\n",
        "for i in xpred1:\n",
        "    if i[0]>i[1]:\n",
        "        xpred.append(0)\n",
        "    else:\n",
        "        xpred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(xpred, y_train1))\n",
        "print('roc_auc_score', roc_auc_score(xpred, y_train1))\n",
        "print('precision_score', precision_score(xpred, y_train1))\n",
        "\n",
        "print('recall_score', recall_score(xpred, y_train1))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rct_Temperature_Readings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}