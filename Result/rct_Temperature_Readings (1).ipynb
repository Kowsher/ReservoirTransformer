{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M2icok5AYzvS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/IOT-temp 2.csv')\n",
        "\n",
        "df=df.drop(columns = ['id', 'room_id/id', 'noted_date'])\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "#df = imputer.fit_transform(df)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1= LabelEncoder()\n",
        "\n",
        "\n",
        "df['out/in']= le1.fit_transform(df['out/in'])\n",
        "\n",
        "'''\n",
        "df=df.dropna()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "'''\n",
        "\n",
        "\n",
        "#df['icon']= le.fit_transform(df['icon'])\n",
        "\n",
        "y = df['out/in'].values\n",
        "df=df.drop(columns = ['out/in'])\n",
        "X = df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guBeZhMnFYBm",
        "outputId": "3c74bfd3-2017-42c2-eef8-201318557677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "CgyrLr_T-2Ok",
        "outputId": "834b274e-42df-4db7-eee3-012996241fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(594, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_82uIQCFdziZ",
        "outputId": "71bcf31f-8d2e-4ce3-c12a-d65622ced220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 20345, 1: 77261})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Rn2VoK1owI5"
      },
      "outputs": [],
      "source": [
        "\n",
        "y=y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X=np.concatenate((X[1:], y[0:-1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxWNccXgu12l",
        "outputId": "4b71c02b-8699-4065-c98b-1e9f6d83f5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=1fe27358151aa3f10e6a50c9999429b53a0b6c6855aa9b3c941a07da47be124f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8iIZ1105tFnf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X= sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl0SFJ5WtbBP",
        "outputId": "a7579939-128f-46bd-d834-d6004d8e1702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrcn\n",
            "  Downloading PyRCN-0.0.16.post1-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81 kB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyrcn\n",
            "Successfully installed pyrcn-0.0.16.post1\n"
          ]
        }
      ],
      "source": [
        "pip install pyrcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gEPWGptbuTYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y[1:], test_size=0.2, shuffle=False, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDAxxhLupYK6",
        "outputId": "3c343778-2705-4f33-89b7-e9047657317d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78084, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1rOeQ0-ptl_G"
      },
      "outputs": [],
      "source": [
        "from pyrcn.base.blocks import InputToNode\n",
        "from sklearn . datasets import make_blobs\n",
        "# Generate a toy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tShY5ktnrN",
        "outputId": "6e831e6e-450b-4b15-ccfa-278eed30ed49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=8 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "input_to_node = InputToNode (8, input_activation='relu',input_scaling =1.0 )\n",
        "\n",
        "\n",
        "x_train= input_to_node.fit_transform (x_train1)\n",
        "x_test= input_to_node.transform (x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDw3vyTRuB-o",
        "outputId": "7c4858f5-1cae-4a82-d35e-f9cac1e34048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=8 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from pyrcn.base.blocks import NodeToNode\n",
        "node_to_node = NodeToNode (8, reservoir_activation='relu', spectral_radius =1.0 , leakage =0.4 ,bidirectional = False )\n",
        "x_train=node_to_node . fit_transform(x_train)\n",
        "x_test= node_to_node.transform (x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b-dqEbXtuCpY"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "x_train1 = x_train1.reshape((x_train1.shape[0], x_train1.shape[1], 1))\n",
        "x_test1 = x_test1.reshape((x_test1.shape[0], x_test1.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qi0MhVmsuswU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NX3w9IqBuuZ-"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HsU6bO-PuvqD"
      },
      "outputs": [],
      "source": [
        "from keras_self_attention import SeqSelfAttention\n",
        "def build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputsy = keras.Input(shape=input_shapey)\n",
        "    inputsz = keras.Input(shape=input_shapez)\n",
        "    y = inputsy\n",
        "    z= inputsz\n",
        "\n",
        "    #z=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(z)\n",
        "\n",
        "    #y=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(y)\n",
        "\n",
        "    z=SeqSelfAttention()(z)\n",
        "\n",
        "    #y=SeqSelfAttention()(y)\n",
        "\n",
        "\n",
        "\n",
        "    x=layers.Concatenate(axis=1)([y, z])\n",
        "    #x=layers.Add()([inputsy, inputsz])\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    #outputs = layers.Dense(1)(x)\n",
        "    return keras.Model([inputsy,inputsz], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Hu3CH4vZzu",
        "outputId": "cfabecce-c30b-4b75-b569-19250ec21022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 8, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 2, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " seq_self_attention (SeqSelfAtt  (None, 8, 1)        129         ['input_2[0][0]']                \n",
            " ention)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 10, 1)        0           ['input_1[0][0]',                \n",
            "                                                                  'seq_self_attention[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 10, 1)       2           ['concatenate[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 10, 1)       7169        ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 10, 1)        0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 10, 1)       0           ['dropout[0][0]',                \n",
            " da)                                                              'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 10, 4)        8           ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 10, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 10, 1)        5           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 10, 1)       0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 10, 1)       7169        ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 10, 1)       0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 10, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 10, 1)        5           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 10, 1)       0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 10, 1)       7169        ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 10, 1)       0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 10, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 10, 1)        5           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 10, 1)       0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 10, 1)       7169        ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 10, 1)       0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 10, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 10, 1)        5           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 10, 1)       0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 10)          0           ['tf.__operators__.add_7[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          1408        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            258         ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 30,539\n",
            "Trainable params: 30,539\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shapey = x_train1.shape[1:]\n",
        "input_shapez = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"mean_absolute_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"mean_absolute_error\"],\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jta9P_Q3vkrc",
        "outputId": "02696bcd-317f-4c5f-b0ed-43569994482d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "611/611 [==============================] - 17s 19ms/step - loss: 0.2656 - sparse_categorical_accuracy: 0.8783 - val_loss: 0.5145 - val_sparse_categorical_accuracy: 0.7327\n",
            "Epoch 2/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1857 - sparse_categorical_accuracy: 0.9131 - val_loss: 0.4788 - val_sparse_categorical_accuracy: 0.7412\n",
            "Epoch 3/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1584 - sparse_categorical_accuracy: 0.9289 - val_loss: 0.4649 - val_sparse_categorical_accuracy: 0.7532\n",
            "Epoch 4/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1453 - sparse_categorical_accuracy: 0.9326 - val_loss: 0.4610 - val_sparse_categorical_accuracy: 0.7674\n",
            "Epoch 5/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1409 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.4615 - val_sparse_categorical_accuracy: 0.7708\n",
            "Epoch 6/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9353 - val_loss: 0.4704 - val_sparse_categorical_accuracy: 0.7565\n",
            "Epoch 7/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1363 - sparse_categorical_accuracy: 0.9379 - val_loss: 0.4700 - val_sparse_categorical_accuracy: 0.7638\n",
            "Epoch 8/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1346 - sparse_categorical_accuracy: 0.9372 - val_loss: 0.5007 - val_sparse_categorical_accuracy: 0.7427\n",
            "Epoch 9/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1343 - sparse_categorical_accuracy: 0.9376 - val_loss: 0.4723 - val_sparse_categorical_accuracy: 0.7695\n",
            "Epoch 10/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1329 - sparse_categorical_accuracy: 0.9384 - val_loss: 0.4662 - val_sparse_categorical_accuracy: 0.7703\n",
            "Epoch 11/100\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.1323 - sparse_categorical_accuracy: 0.9383 - val_loss: 0.4617 - val_sparse_categorical_accuracy: 0.7694\n",
            "Epoch 12/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1312 - sparse_categorical_accuracy: 0.9387 - val_loss: 0.4622 - val_sparse_categorical_accuracy: 0.7659\n",
            "Epoch 13/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1317 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.4586 - val_sparse_categorical_accuracy: 0.7671\n",
            "Epoch 14/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1310 - sparse_categorical_accuracy: 0.9391 - val_loss: 0.4641 - val_sparse_categorical_accuracy: 0.7655\n",
            "Epoch 15/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1305 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.4670 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 16/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1305 - sparse_categorical_accuracy: 0.9393 - val_loss: 0.4588 - val_sparse_categorical_accuracy: 0.7731\n",
            "Epoch 17/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1298 - sparse_categorical_accuracy: 0.9391 - val_loss: 0.4707 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 18/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1294 - sparse_categorical_accuracy: 0.9392 - val_loss: 0.4622 - val_sparse_categorical_accuracy: 0.7687\n",
            "Epoch 19/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1292 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.4651 - val_sparse_categorical_accuracy: 0.7643\n",
            "Epoch 20/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1284 - sparse_categorical_accuracy: 0.9391 - val_loss: 0.4632 - val_sparse_categorical_accuracy: 0.7712\n",
            "Epoch 21/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1282 - sparse_categorical_accuracy: 0.9398 - val_loss: 0.4590 - val_sparse_categorical_accuracy: 0.7700\n",
            "Epoch 22/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1284 - sparse_categorical_accuracy: 0.9399 - val_loss: 0.4601 - val_sparse_categorical_accuracy: 0.7696\n",
            "Epoch 23/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1278 - sparse_categorical_accuracy: 0.9407 - val_loss: 0.4571 - val_sparse_categorical_accuracy: 0.7727\n",
            "Epoch 24/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1281 - sparse_categorical_accuracy: 0.9408 - val_loss: 0.4653 - val_sparse_categorical_accuracy: 0.7635\n",
            "Epoch 25/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1282 - sparse_categorical_accuracy: 0.9407 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.7697\n",
            "Epoch 26/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1280 - sparse_categorical_accuracy: 0.9395 - val_loss: 0.4602 - val_sparse_categorical_accuracy: 0.7736\n",
            "Epoch 27/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1284 - sparse_categorical_accuracy: 0.9403 - val_loss: 0.4674 - val_sparse_categorical_accuracy: 0.7663\n",
            "Epoch 28/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1280 - sparse_categorical_accuracy: 0.9403 - val_loss: 0.4567 - val_sparse_categorical_accuracy: 0.7695\n",
            "Epoch 29/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1272 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.4698 - val_sparse_categorical_accuracy: 0.7619\n",
            "Epoch 30/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1265 - sparse_categorical_accuracy: 0.9404 - val_loss: 0.4592 - val_sparse_categorical_accuracy: 0.7660\n",
            "Epoch 31/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1264 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.4619 - val_sparse_categorical_accuracy: 0.7652\n",
            "Epoch 32/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1268 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.4580 - val_sparse_categorical_accuracy: 0.7712\n",
            "Epoch 33/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1263 - sparse_categorical_accuracy: 0.9418 - val_loss: 0.4629 - val_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 34/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1257 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.4665 - val_sparse_categorical_accuracy: 0.7708\n",
            "Epoch 35/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9414 - val_loss: 0.4580 - val_sparse_categorical_accuracy: 0.7702\n",
            "Epoch 36/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1253 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.4563 - val_sparse_categorical_accuracy: 0.7727\n",
            "Epoch 37/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1259 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.4640 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 38/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1269 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.4651 - val_sparse_categorical_accuracy: 0.7657\n",
            "Epoch 39/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1248 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.4920 - val_sparse_categorical_accuracy: 0.7690\n",
            "Epoch 40/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.9409 - val_loss: 0.4599 - val_sparse_categorical_accuracy: 0.7688\n",
            "Epoch 41/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1246 - sparse_categorical_accuracy: 0.9415 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.7726\n",
            "Epoch 42/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1251 - sparse_categorical_accuracy: 0.9417 - val_loss: 0.4616 - val_sparse_categorical_accuracy: 0.7687\n",
            "Epoch 43/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1257 - sparse_categorical_accuracy: 0.9414 - val_loss: 0.4625 - val_sparse_categorical_accuracy: 0.7731\n",
            "Epoch 44/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1251 - sparse_categorical_accuracy: 0.9419 - val_loss: 0.4553 - val_sparse_categorical_accuracy: 0.7715\n",
            "Epoch 45/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1244 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.4718 - val_sparse_categorical_accuracy: 0.7636\n",
            "Epoch 46/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1249 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.7728\n",
            "Epoch 47/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1243 - sparse_categorical_accuracy: 0.9411 - val_loss: 0.4612 - val_sparse_categorical_accuracy: 0.7686\n",
            "Epoch 48/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1242 - sparse_categorical_accuracy: 0.9421 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 49/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1248 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.4536 - val_sparse_categorical_accuracy: 0.7704\n",
            "Epoch 50/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1241 - sparse_categorical_accuracy: 0.9422 - val_loss: 0.4557 - val_sparse_categorical_accuracy: 0.7752\n",
            "Epoch 51/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9409 - val_loss: 0.4622 - val_sparse_categorical_accuracy: 0.7694\n",
            "Epoch 52/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1241 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.4533 - val_sparse_categorical_accuracy: 0.7700\n",
            "Epoch 53/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1244 - sparse_categorical_accuracy: 0.9422 - val_loss: 0.4592 - val_sparse_categorical_accuracy: 0.7678\n",
            "Epoch 54/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1237 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.4512 - val_sparse_categorical_accuracy: 0.7753\n",
            "Epoch 55/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1237 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.4531 - val_sparse_categorical_accuracy: 0.7738\n",
            "Epoch 56/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1237 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.4600 - val_sparse_categorical_accuracy: 0.7686\n",
            "Epoch 57/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.4568 - val_sparse_categorical_accuracy: 0.7699\n",
            "Epoch 58/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9423 - val_loss: 0.4614 - val_sparse_categorical_accuracy: 0.7727\n",
            "Epoch 59/100\n",
            "611/611 [==============================] - 14s 22ms/step - loss: 0.1236 - sparse_categorical_accuracy: 0.9419 - val_loss: 0.4482 - val_sparse_categorical_accuracy: 0.7753\n",
            "Epoch 60/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1231 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.4556 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 61/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.4572 - val_sparse_categorical_accuracy: 0.7732\n",
            "Epoch 62/100\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.1227 - sparse_categorical_accuracy: 0.9427 - val_loss: 0.4493 - val_sparse_categorical_accuracy: 0.7756\n",
            "Epoch 63/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1231 - sparse_categorical_accuracy: 0.9424 - val_loss: 0.4489 - val_sparse_categorical_accuracy: 0.7774\n",
            "Epoch 64/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1234 - sparse_categorical_accuracy: 0.9418 - val_loss: 0.4488 - val_sparse_categorical_accuracy: 0.7754\n",
            "Epoch 65/100\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.1235 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.4562 - val_sparse_categorical_accuracy: 0.7747\n",
            "Epoch 66/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1231 - sparse_categorical_accuracy: 0.9422 - val_loss: 0.4627 - val_sparse_categorical_accuracy: 0.7713\n",
            "Epoch 67/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.9426 - val_loss: 0.4829 - val_sparse_categorical_accuracy: 0.7660\n",
            "Epoch 68/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1227 - sparse_categorical_accuracy: 0.9426 - val_loss: 0.4496 - val_sparse_categorical_accuracy: 0.7760\n",
            "Epoch 69/100\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.9418 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.7729\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "history=model.fit(\n",
        "    [x_train1,x_train],\n",
        "    y_train1,\n",
        "    validation_data=([x_test1, x_test], y_test1),\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42doJdzh7mKo",
        "outputId": "81ed77d8-3e22-46eb-f281-6be86b26046e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7752676604682137\n",
            "f1_score 0.8278460149903857\n",
            "jaccard_score 0.7062604620020088\n",
            "roc_auc_score 0.7720873933912126\n",
            "precision_score 0.8814975764666555\n",
            "recall_score 0.780350669527262\n"
          ]
        }
      ],
      "source": [
        "#testing prediction\n",
        "\n",
        "#testing prediction\n",
        "ypred1=model.predict([x_test1, x_test])\n",
        "\n",
        "ypred=[]\n",
        "#testing prediction\n",
        "for i in ypred1:\n",
        "    if i[0]>i[1]:\n",
        "        ypred.append(0)\n",
        "    else:\n",
        "        ypred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print(accuracy_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(ypred, y_test1))\n",
        "print('roc_auc_score', roc_auc_score(ypred, y_test1))\n",
        "print('precision_score', precision_score(ypred, y_test1))\n",
        "\n",
        "print('recall_score', recall_score(ypred, y_test1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9Rqyi5DuRJ",
        "outputId": "a36e721e-6e24-47fb-fa1b-9f9fbfa04dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9451232006557041\n",
            "f1_score 0.9676859846913767\n",
            "jaccard_score 0.93739498867704\n",
            "roc_auc_score 0.9239277003461979\n",
            "precision_score 0.9826173520177656\n",
            "recall_score 0.9532016045164166\n"
          ]
        }
      ],
      "source": [
        "#training prediction\n",
        "xpred1=model.predict([x_train1, x_train])\n",
        "xpred=[]\n",
        "\n",
        "for i in xpred1:\n",
        "    if i[0]>i[1]:\n",
        "        xpred.append(0)\n",
        "    else:\n",
        "        xpred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(xpred, y_train1))\n",
        "print('roc_auc_score', roc_auc_score(xpred, y_train1))\n",
        "print('precision_score', precision_score(xpred, y_train1))\n",
        "\n",
        "print('recall_score', recall_score(xpred, y_train1))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rct_Temperature_Readings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}