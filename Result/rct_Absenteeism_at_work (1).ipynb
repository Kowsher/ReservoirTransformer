{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M2icok5AYzvS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Absenteeism_at_work.csv', sep =';')\n",
        "\n",
        "#df=df.drop(columns = ['date'])\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "#df = imputer.fit_transform(df)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1= LabelEncoder()\n",
        "le2= LabelEncoder()\n",
        "le3= LabelEncoder()\n",
        "\n",
        "'''\n",
        "df=df.dropna()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "'''\n",
        "\n",
        "\n",
        "#df['icon']= le.fit_transform(df['icon'])\n",
        "\n",
        "y = df['Social drinker'].values\n",
        "df=df.drop(columns = ['Social drinker'])\n",
        "X = df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "guBeZhMnFYBm",
        "outputId": "99f54481-ea31-4364-fed8-c8da6206a883"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     ID  Reason for absence  Month of absence  Day of the week  Seasons  \\\n",
              "0    11                  26                 7                3        1   \n",
              "1    36                   0                 7                3        1   \n",
              "2     3                  23                 7                4        1   \n",
              "3     7                   7                 7                5        1   \n",
              "4    11                  23                 7                5        1   \n",
              "..   ..                 ...               ...              ...      ...   \n",
              "735  11                  14                 7                3        1   \n",
              "736   1                  11                 7                3        1   \n",
              "737   4                   0                 0                3        1   \n",
              "738   8                   0                 0                4        2   \n",
              "739  35                   0                 0                6        3   \n",
              "\n",
              "     Transportation expense  Distance from Residence to Work  Service time  \\\n",
              "0                       289                               36            13   \n",
              "1                       118                               13            18   \n",
              "2                       179                               51            18   \n",
              "3                       279                                5            14   \n",
              "4                       289                               36            13   \n",
              "..                      ...                              ...           ...   \n",
              "735                     289                               36            13   \n",
              "736                     235                               11            14   \n",
              "737                     118                               14            13   \n",
              "738                     231                               35            14   \n",
              "739                     179                               45            14   \n",
              "\n",
              "     Age  Work load Average/day   Hit target  Disciplinary failure  Education  \\\n",
              "0     33                 239.554          97                     0          1   \n",
              "1     50                 239.554          97                     1          1   \n",
              "2     38                 239.554          97                     0          1   \n",
              "3     39                 239.554          97                     0          1   \n",
              "4     33                 239.554          97                     0          1   \n",
              "..   ...                     ...         ...                   ...        ...   \n",
              "735   33                 264.604          93                     0          1   \n",
              "736   37                 264.604          93                     0          3   \n",
              "737   40                 271.219          95                     0          1   \n",
              "738   39                 271.219          95                     0          1   \n",
              "739   53                 271.219          95                     0          1   \n",
              "\n",
              "     Son  Social smoker  Pet  Weight  Height  Body mass index  \\\n",
              "0      2              0    1      90     172               30   \n",
              "1      1              0    0      98     178               31   \n",
              "2      0              0    0      89     170               31   \n",
              "3      2              1    0      68     168               24   \n",
              "4      2              0    1      90     172               30   \n",
              "..   ...            ...  ...     ...     ...              ...   \n",
              "735    2              0    1      90     172               30   \n",
              "736    1              0    1      88     172               29   \n",
              "737    1              0    8      98     170               34   \n",
              "738    2              0    2     100     170               35   \n",
              "739    1              0    1      77     175               25   \n",
              "\n",
              "     Absenteeism time in hours  \n",
              "0                            4  \n",
              "1                            0  \n",
              "2                            2  \n",
              "3                            4  \n",
              "4                            2  \n",
              "..                         ...  \n",
              "735                          8  \n",
              "736                          4  \n",
              "737                          0  \n",
              "738                          0  \n",
              "739                          0  \n",
              "\n",
              "[740 rows x 20 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5033ab6d-34a7-41b2-b276-7bc1465d4a47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Reason for absence</th>\n",
              "      <th>Month of absence</th>\n",
              "      <th>Day of the week</th>\n",
              "      <th>Seasons</th>\n",
              "      <th>Transportation expense</th>\n",
              "      <th>Distance from Residence to Work</th>\n",
              "      <th>Service time</th>\n",
              "      <th>Age</th>\n",
              "      <th>Work load Average/day</th>\n",
              "      <th>Hit target</th>\n",
              "      <th>Disciplinary failure</th>\n",
              "      <th>Education</th>\n",
              "      <th>Son</th>\n",
              "      <th>Social smoker</th>\n",
              "      <th>Pet</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Height</th>\n",
              "      <th>Body mass index</th>\n",
              "      <th>Absenteeism time in hours</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>36</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>239.554</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>172</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>118</td>\n",
              "      <td>13</td>\n",
              "      <td>18</td>\n",
              "      <td>50</td>\n",
              "      <td>239.554</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>98</td>\n",
              "      <td>178</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>179</td>\n",
              "      <td>51</td>\n",
              "      <td>18</td>\n",
              "      <td>38</td>\n",
              "      <td>239.554</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>170</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>279</td>\n",
              "      <td>5</td>\n",
              "      <td>14</td>\n",
              "      <td>39</td>\n",
              "      <td>239.554</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>168</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>36</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>239.554</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>172</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>36</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>264.604</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>172</td>\n",
              "      <td>30</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>235</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>37</td>\n",
              "      <td>264.604</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>172</td>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>118</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>40</td>\n",
              "      <td>271.219</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>98</td>\n",
              "      <td>170</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>231</td>\n",
              "      <td>35</td>\n",
              "      <td>14</td>\n",
              "      <td>39</td>\n",
              "      <td>271.219</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>170</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>179</td>\n",
              "      <td>45</td>\n",
              "      <td>14</td>\n",
              "      <td>53</td>\n",
              "      <td>271.219</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>77</td>\n",
              "      <td>175</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>740 rows × 20 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5033ab6d-34a7-41b2-b276-7bc1465d4a47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5033ab6d-34a7-41b2-b276-7bc1465d4a47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5033ab6d-34a7-41b2-b276-7bc1465d4a47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "CgyrLr_T-2Ok",
        "outputId": "49424d0e-7eaf-469c-db9b-5262ff90096c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(740, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(df['Disciplinary failure'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_82uIQCFdziZ",
        "outputId": "3bf724c5-5fe3-4862-c802-06a495c13615"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 700, 1: 40})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0Rn2VoK1owI5"
      },
      "outputs": [],
      "source": [
        "\n",
        "y=y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X=np.concatenate((X[1:], y[0:-1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxWNccXgu12l",
        "outputId": "ff39f523-0775-48a1-8c43-7d36352be1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=2c2a45adcd365ad58a47526dbd6c2f61ed67d70d3f8e83364407db02f1df1bb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8iIZ1105tFnf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X= sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl0SFJ5WtbBP",
        "outputId": "cb2ef0f0-6c1d-405e-8365-4d5b4f909c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrcn\n",
            "  Downloading PyRCN-0.0.16.post1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 3.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyrcn\n",
            "Successfully installed pyrcn-0.0.16.post1\n"
          ]
        }
      ],
      "source": [
        "pip install pyrcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gEPWGptbuTYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y[1:], test_size=0.2, shuffle=False, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDAxxhLupYK6",
        "outputId": "2adeedd3-cd73-40e5-81c1-69034e5f3963"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(591, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "x_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1rOeQ0-ptl_G"
      },
      "outputs": [],
      "source": [
        "from pyrcn.base.blocks import InputToNode\n",
        "from sklearn . datasets import make_blobs\n",
        "# Generate a toy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tShY5ktnrN",
        "outputId": "a9edbbb7-9e4e-4c9f-9a8a-fdad8dfd4ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=50 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "input_to_node = InputToNode (50, input_activation='relu',input_scaling =1.0 )\n",
        "\n",
        "\n",
        "x_train= input_to_node.fit_transform (x_train1)\n",
        "x_test= input_to_node.transform (x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDw3vyTRuB-o",
        "outputId": "e1f4c491-1260-4fac-fc6f-7f555b2de209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=50 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from pyrcn.base.blocks import NodeToNode\n",
        "node_to_node = NodeToNode (50, reservoir_activation='relu', spectral_radius =0.99 , leakage =0.2 ,bidirectional = False )\n",
        "x_train=node_to_node . fit_transform(x_train)\n",
        "x_test= node_to_node.transform (x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b-dqEbXtuCpY"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "x_train1 = x_train1.reshape((x_train1.shape[0], x_train1.shape[1], 1))\n",
        "x_test1 = x_test1.reshape((x_test1.shape[0], x_test1.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qi0MhVmsuswU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NX3w9IqBuuZ-"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HsU6bO-PuvqD"
      },
      "outputs": [],
      "source": [
        "from keras_self_attention import SeqSelfAttention\n",
        "def build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputsy = keras.Input(shape=input_shapey)\n",
        "    inputsz = keras.Input(shape=input_shapez)\n",
        "    y = inputsy\n",
        "    z= inputsz\n",
        "\n",
        "    #z=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(z)\n",
        "\n",
        "    #y=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(y)\n",
        "\n",
        "    z=SeqSelfAttention()(z)\n",
        "    z=layers.Flatten()(z)\n",
        "    \n",
        "    z=layers.Dense(5, activation=\"relu\")(z)\n",
        "    z= layers.Reshape((-1,1))(z)\n",
        "    #y=SeqSelfAttention()(y)\n",
        "\n",
        "\n",
        "\n",
        "    x=layers.Concatenate(axis=1)([y, z])\n",
        "    #x=layers.Add()([inputsy, inputsz])\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    #outputs = layers.Dense(1)(x)\n",
        "    return keras.Model([inputsy,inputsz], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Hu3CH4vZzu",
        "outputId": "b2b6b5c9-857e-4493-aea7-002fa3f7273e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 50, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " seq_self_attention (SeqSelfAtt  (None, 50, 1)       129         ['input_2[0][0]']                \n",
            " ention)                                                                                          \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 50)           0           ['seq_self_attention[0][0]']     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            255         ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 21, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 5, 1)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 26, 1)        0           ['input_1[0][0]',                \n",
            "                                                                  'reshape[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 26, 1)       2           ['concatenate[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 26, 1)       71          ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 26, 1)        0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 26, 1)       0           ['dropout[0][0]',                \n",
            " da)                                                              'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 26, 1)       2           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 26, 2)        4           ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 26, 2)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 26, 1)        3           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 26, 1)       0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 26)          0           ['tf.__operators__.add_1[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 64)           1728        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 2)            130         ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,324\n",
            "Trainable params: 2,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shapey = x_train1.shape[1:]\n",
        "input_shapez = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size=5,\n",
        "    num_heads=2,\n",
        "    ff_dim=2,\n",
        "    num_transformer_blocks=1,\n",
        "    mlp_units=[64],\n",
        "    mlp_dropout=0.8,\n",
        "    dropout=0.8,\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"mean_absolute_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"mean_absolute_error\"],\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jta9P_Q3vkrc",
        "outputId": "e5626dbc-550b-416c-dd03-83ab1f89c9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0136 - accuracy: 0.9983 - val_loss: 1.8408 - val_accuracy: 0.9459\n",
            "Epoch 2/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0185 - accuracy: 0.9932 - val_loss: 1.8552 - val_accuracy: 0.9459\n",
            "Epoch 3/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0313 - accuracy: 0.9865 - val_loss: 1.7275 - val_accuracy: 0.9459\n",
            "Epoch 4/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0142 - accuracy: 0.9915 - val_loss: 1.6543 - val_accuracy: 0.9459\n",
            "Epoch 5/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0207 - accuracy: 0.9898 - val_loss: 1.6070 - val_accuracy: 0.9459\n",
            "Epoch 6/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0359 - accuracy: 0.9848 - val_loss: 1.6099 - val_accuracy: 0.9459\n",
            "Epoch 7/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0381 - accuracy: 0.9814 - val_loss: 1.6003 - val_accuracy: 0.9459\n",
            "Epoch 8/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0307 - accuracy: 0.9915 - val_loss: 1.6144 - val_accuracy: 0.9459\n",
            "Epoch 9/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 0.9949 - val_loss: 1.6433 - val_accuracy: 0.9459\n",
            "Epoch 10/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0169 - accuracy: 0.9983 - val_loss: 1.6710 - val_accuracy: 0.9459\n",
            "Epoch 11/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0226 - accuracy: 0.9898 - val_loss: 1.6829 - val_accuracy: 0.9459\n",
            "Epoch 12/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0141 - accuracy: 0.9932 - val_loss: 1.6994 - val_accuracy: 0.9459\n",
            "Epoch 13/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0112 - accuracy: 0.9949 - val_loss: 1.7032 - val_accuracy: 0.9459\n",
            "Epoch 14/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0188 - accuracy: 0.9898 - val_loss: 1.7079 - val_accuracy: 0.9459\n",
            "Epoch 15/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9932 - val_loss: 1.7267 - val_accuracy: 0.9459\n",
            "Epoch 16/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.7284 - val_accuracy: 0.9459\n",
            "Epoch 17/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 1.6987 - val_accuracy: 0.9459\n",
            "Epoch 18/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0157 - accuracy: 0.9915 - val_loss: 1.7335 - val_accuracy: 0.9459\n",
            "Epoch 19/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0219 - accuracy: 0.9898 - val_loss: 1.7831 - val_accuracy: 0.9459\n",
            "Epoch 20/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 0.9949 - val_loss: 1.7993 - val_accuracy: 0.9459\n",
            "Epoch 21/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 0.9966 - val_loss: 1.8214 - val_accuracy: 0.9459\n",
            "Epoch 22/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0323 - accuracy: 0.9865 - val_loss: 1.7423 - val_accuracy: 0.9459\n",
            "Epoch 23/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0259 - accuracy: 0.9932 - val_loss: 1.6722 - val_accuracy: 0.9459\n",
            "Epoch 24/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0138 - accuracy: 0.9932 - val_loss: 1.6045 - val_accuracy: 0.9459\n",
            "Epoch 25/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0171 - accuracy: 0.9932 - val_loss: 1.5940 - val_accuracy: 0.9459\n",
            "Epoch 26/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0250 - accuracy: 0.9865 - val_loss: 1.6082 - val_accuracy: 0.9459\n",
            "Epoch 27/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0171 - accuracy: 0.9915 - val_loss: 1.6191 - val_accuracy: 0.9459\n",
            "Epoch 28/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0115 - accuracy: 0.9949 - val_loss: 1.6420 - val_accuracy: 0.9459\n",
            "Epoch 29/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0183 - accuracy: 0.9915 - val_loss: 1.6568 - val_accuracy: 0.9459\n",
            "Epoch 30/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9932 - val_loss: 1.6615 - val_accuracy: 0.9459\n",
            "Epoch 31/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0277 - accuracy: 0.9848 - val_loss: 1.6658 - val_accuracy: 0.9459\n",
            "Epoch 32/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 1.6465 - val_accuracy: 0.9459\n",
            "Epoch 33/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 1.6518 - val_accuracy: 0.9459\n",
            "Epoch 34/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0357 - accuracy: 0.9898 - val_loss: 1.6217 - val_accuracy: 0.9459\n",
            "Epoch 35/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0227 - accuracy: 0.9898 - val_loss: 1.5924 - val_accuracy: 0.9459\n",
            "Epoch 36/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0183 - accuracy: 0.9882 - val_loss: 1.6007 - val_accuracy: 0.9459\n",
            "Epoch 37/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0152 - accuracy: 0.9932 - val_loss: 1.6452 - val_accuracy: 0.9459\n",
            "Epoch 38/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9932 - val_loss: 1.6969 - val_accuracy: 0.9459\n",
            "Epoch 39/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 1.7412 - val_accuracy: 0.9459\n",
            "Epoch 40/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9966 - val_loss: 1.7664 - val_accuracy: 0.9459\n",
            "Epoch 41/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 0.9949 - val_loss: 1.7709 - val_accuracy: 0.9459\n",
            "Epoch 42/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0105 - accuracy: 0.9932 - val_loss: 1.7730 - val_accuracy: 0.9459\n",
            "Epoch 43/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9932 - val_loss: 1.7618 - val_accuracy: 0.9459\n",
            "Epoch 44/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0239 - accuracy: 0.9932 - val_loss: 1.7498 - val_accuracy: 0.9459\n",
            "Epoch 45/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0157 - accuracy: 0.9898 - val_loss: 1.7380 - val_accuracy: 0.9459\n",
            "Epoch 46/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0425 - accuracy: 0.9898 - val_loss: 1.7284 - val_accuracy: 0.9459\n",
            "Epoch 47/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0102 - accuracy: 0.9949 - val_loss: 1.7521 - val_accuracy: 0.9459\n",
            "Epoch 48/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 1.8057 - val_accuracy: 0.9459\n",
            "Epoch 49/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0089 - accuracy: 0.9983 - val_loss: 1.8601 - val_accuracy: 0.9459\n",
            "Epoch 50/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0461 - accuracy: 0.9865 - val_loss: 1.8364 - val_accuracy: 0.9459\n",
            "Epoch 51/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9932 - val_loss: 1.7957 - val_accuracy: 0.9459\n",
            "Epoch 52/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0127 - accuracy: 0.9966 - val_loss: 1.7395 - val_accuracy: 0.9459\n",
            "Epoch 53/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0293 - accuracy: 0.9882 - val_loss: 1.7161 - val_accuracy: 0.9459\n",
            "Epoch 54/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0281 - accuracy: 0.9848 - val_loss: 1.7273 - val_accuracy: 0.9459\n",
            "Epoch 55/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0243 - accuracy: 0.9865 - val_loss: 1.7584 - val_accuracy: 0.9459\n",
            "Epoch 56/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0249 - accuracy: 0.9932 - val_loss: 1.7881 - val_accuracy: 0.9459\n",
            "Epoch 57/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 1.8090 - val_accuracy: 0.9459\n",
            "Epoch 58/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 1.8227 - val_accuracy: 0.9459\n",
            "Epoch 59/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0149 - accuracy: 0.9915 - val_loss: 1.8386 - val_accuracy: 0.9459\n",
            "Epoch 60/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0214 - accuracy: 0.9898 - val_loss: 1.8344 - val_accuracy: 0.9459\n",
            "Epoch 61/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0316 - accuracy: 0.9865 - val_loss: 1.8082 - val_accuracy: 0.9459\n",
            "Epoch 62/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0234 - accuracy: 0.9915 - val_loss: 1.7443 - val_accuracy: 0.9459\n",
            "Epoch 63/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 1.7213 - val_accuracy: 0.9459\n",
            "Epoch 64/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 1.7327 - val_accuracy: 0.9459\n",
            "Epoch 65/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0261 - accuracy: 0.9915 - val_loss: 1.7469 - val_accuracy: 0.9459\n",
            "Epoch 66/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0297 - accuracy: 0.9898 - val_loss: 1.7665 - val_accuracy: 0.9459\n",
            "Epoch 67/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0166 - accuracy: 0.9949 - val_loss: 1.7791 - val_accuracy: 0.9459\n",
            "Epoch 68/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0150 - accuracy: 0.9932 - val_loss: 1.8101 - val_accuracy: 0.9459\n",
            "Epoch 69/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 1.8058 - val_accuracy: 0.9459\n",
            "Epoch 70/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 1.7547 - val_accuracy: 0.9459\n",
            "Epoch 71/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0359 - accuracy: 0.9865 - val_loss: 1.7101 - val_accuracy: 0.9459\n",
            "Epoch 72/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0119 - accuracy: 0.9983 - val_loss: 1.7238 - val_accuracy: 0.9459\n",
            "Epoch 73/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0193 - accuracy: 0.9915 - val_loss: 1.7621 - val_accuracy: 0.9459\n",
            "Epoch 74/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0157 - accuracy: 0.9966 - val_loss: 1.8098 - val_accuracy: 0.9459\n",
            "Epoch 75/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9932 - val_loss: 1.8319 - val_accuracy: 0.9459\n",
            "Epoch 76/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 1.8548 - val_accuracy: 0.9459\n",
            "Epoch 77/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 1.8708 - val_accuracy: 0.9459\n",
            "Epoch 78/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0229 - accuracy: 0.9949 - val_loss: 1.8612 - val_accuracy: 0.9459\n",
            "Epoch 79/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0272 - accuracy: 0.9882 - val_loss: 1.8891 - val_accuracy: 0.9459\n",
            "Epoch 80/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0137 - accuracy: 0.9932 - val_loss: 1.9254 - val_accuracy: 0.9459\n",
            "Epoch 81/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0147 - accuracy: 0.9932 - val_loss: 1.9664 - val_accuracy: 0.9459\n",
            "Epoch 82/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0254 - accuracy: 0.9898 - val_loss: 1.9372 - val_accuracy: 0.9459\n",
            "Epoch 83/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0129 - accuracy: 0.9949 - val_loss: 1.9036 - val_accuracy: 0.9459\n",
            "Epoch 84/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 1.8774 - val_accuracy: 0.9459\n",
            "Epoch 85/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 1.8289 - val_accuracy: 0.9459\n",
            "Epoch 86/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0199 - accuracy: 0.9898 - val_loss: 1.8058 - val_accuracy: 0.9459\n",
            "Epoch 87/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 1.8088 - val_accuracy: 0.9459\n",
            "Epoch 88/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0172 - accuracy: 0.9932 - val_loss: 1.8423 - val_accuracy: 0.9459\n",
            "Epoch 89/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0222 - accuracy: 0.9898 - val_loss: 1.8730 - val_accuracy: 0.9459\n",
            "Epoch 90/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0164 - accuracy: 0.9898 - val_loss: 1.8810 - val_accuracy: 0.9459\n",
            "Epoch 91/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0187 - accuracy: 0.9915 - val_loss: 1.9056 - val_accuracy: 0.9459\n",
            "Epoch 92/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0151 - accuracy: 0.9915 - val_loss: 1.9025 - val_accuracy: 0.9459\n",
            "Epoch 93/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9932 - val_loss: 1.8725 - val_accuracy: 0.9459\n",
            "Epoch 94/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 1.8524 - val_accuracy: 0.9459\n",
            "Epoch 95/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0305 - accuracy: 0.9814 - val_loss: 1.8415 - val_accuracy: 0.9459\n",
            "Epoch 96/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 1.8484 - val_accuracy: 0.9459\n",
            "Epoch 97/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0296 - accuracy: 0.9882 - val_loss: 1.8533 - val_accuracy: 0.9459\n",
            "Epoch 98/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9915 - val_loss: 1.8845 - val_accuracy: 0.9459\n",
            "Epoch 99/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0174 - accuracy: 0.9932 - val_loss: 1.8871 - val_accuracy: 0.9459\n",
            "Epoch 100/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 1.8864 - val_accuracy: 0.9459\n",
            "Epoch 101/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9915 - val_loss: 1.9114 - val_accuracy: 0.9459\n",
            "Epoch 102/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0264 - accuracy: 0.9898 - val_loss: 1.9115 - val_accuracy: 0.9459\n",
            "Epoch 103/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 0.9915 - val_loss: 1.9075 - val_accuracy: 0.9459\n",
            "Epoch 104/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 0.9949 - val_loss: 1.9022 - val_accuracy: 0.9459\n",
            "Epoch 105/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0239 - accuracy: 0.9865 - val_loss: 1.8988 - val_accuracy: 0.9459\n",
            "Epoch 106/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0167 - accuracy: 0.9915 - val_loss: 1.8950 - val_accuracy: 0.9459\n",
            "Epoch 107/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0124 - accuracy: 0.9949 - val_loss: 1.8939 - val_accuracy: 0.9459\n",
            "Epoch 108/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0199 - accuracy: 0.9949 - val_loss: 1.8956 - val_accuracy: 0.9459\n",
            "Epoch 109/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 1.9088 - val_accuracy: 0.9459\n",
            "Epoch 110/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0161 - accuracy: 0.9915 - val_loss: 1.9138 - val_accuracy: 0.9459\n",
            "Epoch 111/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0165 - accuracy: 0.9932 - val_loss: 1.9079 - val_accuracy: 0.9459\n",
            "Epoch 112/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0141 - accuracy: 0.9932 - val_loss: 1.9114 - val_accuracy: 0.9459\n",
            "Epoch 113/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0168 - accuracy: 0.9932 - val_loss: 1.9280 - val_accuracy: 0.9459\n",
            "Epoch 114/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 0.9949 - val_loss: 1.9537 - val_accuracy: 0.9459\n",
            "Epoch 115/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 1.9845 - val_accuracy: 0.9459\n",
            "Epoch 116/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0202 - accuracy: 0.9898 - val_loss: 2.0019 - val_accuracy: 0.9459\n",
            "Epoch 117/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 2.0100 - val_accuracy: 0.9459\n",
            "Epoch 118/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 2.0076 - val_accuracy: 0.9459\n",
            "Epoch 119/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0102 - accuracy: 0.9983 - val_loss: 2.0319 - val_accuracy: 0.9459\n",
            "Epoch 120/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0160 - accuracy: 0.9966 - val_loss: 2.0312 - val_accuracy: 0.9459\n",
            "Epoch 121/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0198 - accuracy: 0.9915 - val_loss: 2.0525 - val_accuracy: 0.9459\n",
            "Epoch 122/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.0644 - val_accuracy: 0.9459\n",
            "Epoch 123/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0158 - accuracy: 0.9915 - val_loss: 2.0851 - val_accuracy: 0.9459\n",
            "Epoch 124/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0206 - accuracy: 0.9882 - val_loss: 2.1098 - val_accuracy: 0.9459\n",
            "Epoch 125/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0333 - accuracy: 0.9865 - val_loss: 2.0752 - val_accuracy: 0.9459\n",
            "Epoch 126/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0226 - accuracy: 0.9898 - val_loss: 2.0094 - val_accuracy: 0.9459\n",
            "Epoch 127/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9966 - val_loss: 1.9381 - val_accuracy: 0.9459\n",
            "Epoch 128/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0188 - accuracy: 0.9898 - val_loss: 1.9047 - val_accuracy: 0.9459\n",
            "Epoch 129/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0175 - accuracy: 0.9915 - val_loss: 1.9091 - val_accuracy: 0.9459\n",
            "Epoch 130/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0134 - accuracy: 0.9949 - val_loss: 1.9758 - val_accuracy: 0.9459\n",
            "Epoch 131/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0121 - accuracy: 0.9949 - val_loss: 2.0416 - val_accuracy: 0.9459\n",
            "Epoch 132/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0095 - accuracy: 0.9949 - val_loss: 2.0899 - val_accuracy: 0.9459\n",
            "Epoch 133/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9932 - val_loss: 2.0929 - val_accuracy: 0.9459\n",
            "Epoch 134/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0127 - accuracy: 0.9966 - val_loss: 2.0949 - val_accuracy: 0.9459\n",
            "Epoch 135/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0250 - accuracy: 0.9831 - val_loss: 2.0915 - val_accuracy: 0.9459\n",
            "Epoch 136/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 2.0861 - val_accuracy: 0.9459\n",
            "Epoch 137/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 2.0709 - val_accuracy: 0.9459\n",
            "Epoch 138/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0098 - accuracy: 0.9966 - val_loss: 2.0375 - val_accuracy: 0.9459\n",
            "Epoch 139/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0208 - accuracy: 0.9882 - val_loss: 2.0374 - val_accuracy: 0.9459\n",
            "Epoch 140/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0182 - accuracy: 0.9898 - val_loss: 2.0579 - val_accuracy: 0.9459\n",
            "Epoch 141/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0187 - accuracy: 0.9882 - val_loss: 2.0645 - val_accuracy: 0.9459\n",
            "Epoch 142/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9932 - val_loss: 2.0835 - val_accuracy: 0.9459\n",
            "Epoch 143/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 2.0737 - val_accuracy: 0.9459\n",
            "Epoch 144/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0237 - accuracy: 0.9915 - val_loss: 2.0496 - val_accuracy: 0.9459\n",
            "Epoch 145/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0195 - accuracy: 0.9915 - val_loss: 2.0700 - val_accuracy: 0.9459\n",
            "Epoch 146/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0169 - accuracy: 0.9915 - val_loss: 2.0677 - val_accuracy: 0.9459\n",
            "Epoch 147/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0125 - accuracy: 0.9949 - val_loss: 2.0614 - val_accuracy: 0.9459\n",
            "Epoch 148/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 2.0559 - val_accuracy: 0.9459\n",
            "Epoch 149/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9949 - val_loss: 2.0744 - val_accuracy: 0.9459\n",
            "Epoch 150/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0192 - accuracy: 0.9898 - val_loss: 2.1237 - val_accuracy: 0.9459\n",
            "Epoch 151/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0247 - accuracy: 0.9898 - val_loss: 2.1390 - val_accuracy: 0.9459\n",
            "Epoch 152/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0202 - accuracy: 0.9932 - val_loss: 2.1937 - val_accuracy: 0.9459\n",
            "Epoch 153/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0151 - accuracy: 0.9932 - val_loss: 2.2611 - val_accuracy: 0.9459\n",
            "Epoch 154/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 2.2895 - val_accuracy: 0.9459\n",
            "Epoch 155/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0326 - accuracy: 0.9898 - val_loss: 2.2673 - val_accuracy: 0.9459\n",
            "Epoch 156/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0207 - accuracy: 0.9898 - val_loss: 2.2234 - val_accuracy: 0.9459\n",
            "Epoch 157/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0364 - accuracy: 0.9932 - val_loss: 2.1965 - val_accuracy: 0.9459\n",
            "Epoch 158/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0397 - accuracy: 0.9865 - val_loss: 2.1855 - val_accuracy: 0.9459\n",
            "Epoch 159/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 2.1829 - val_accuracy: 0.9459\n",
            "Epoch 160/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0191 - accuracy: 0.9898 - val_loss: 2.1306 - val_accuracy: 0.9459\n",
            "Epoch 161/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0143 - accuracy: 0.9949 - val_loss: 2.0904 - val_accuracy: 0.9459\n",
            "Epoch 162/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 2.0868 - val_accuracy: 0.9459\n",
            "Epoch 163/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0148 - accuracy: 0.9949 - val_loss: 2.0860 - val_accuracy: 0.9459\n",
            "Epoch 164/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0176 - accuracy: 0.9949 - val_loss: 2.0801 - val_accuracy: 0.9459\n",
            "Epoch 165/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 2.1043 - val_accuracy: 0.9459\n",
            "Epoch 166/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0244 - accuracy: 0.9882 - val_loss: 2.1262 - val_accuracy: 0.9459\n",
            "Epoch 167/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 2.1591 - val_accuracy: 0.9459\n",
            "Epoch 168/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0161 - accuracy: 0.9932 - val_loss: 2.1928 - val_accuracy: 0.9459\n",
            "Epoch 169/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0152 - accuracy: 0.9932 - val_loss: 2.2158 - val_accuracy: 0.9459\n",
            "Epoch 170/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0146 - accuracy: 0.9915 - val_loss: 2.2311 - val_accuracy: 0.9459\n",
            "Epoch 171/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0199 - accuracy: 0.9882 - val_loss: 2.2638 - val_accuracy: 0.9459\n",
            "Epoch 172/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 2.2790 - val_accuracy: 0.9459\n",
            "Epoch 173/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 2.2879 - val_accuracy: 0.9459\n",
            "Epoch 174/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 2.3003 - val_accuracy: 0.9459\n",
            "Epoch 175/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0253 - accuracy: 0.9848 - val_loss: 2.3143 - val_accuracy: 0.9459\n",
            "Epoch 176/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0198 - accuracy: 0.9882 - val_loss: 2.3420 - val_accuracy: 0.9459\n",
            "Epoch 177/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 2.3618 - val_accuracy: 0.9459\n",
            "Epoch 178/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0200 - accuracy: 0.9932 - val_loss: 2.3619 - val_accuracy: 0.9459\n",
            "Epoch 179/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 2.3578 - val_accuracy: 0.9459\n",
            "Epoch 180/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9915 - val_loss: 2.3403 - val_accuracy: 0.9459\n",
            "Epoch 181/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - accuracy: 0.9983 - val_loss: 2.3256 - val_accuracy: 0.9459\n",
            "Epoch 182/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0228 - accuracy: 0.9865 - val_loss: 2.3421 - val_accuracy: 0.9459\n",
            "Epoch 183/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0104 - accuracy: 0.9949 - val_loss: 2.3589 - val_accuracy: 0.9459\n",
            "Epoch 184/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0197 - accuracy: 0.9898 - val_loss: 2.3556 - val_accuracy: 0.9459\n",
            "Epoch 185/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0138 - accuracy: 0.9932 - val_loss: 2.3636 - val_accuracy: 0.9459\n",
            "Epoch 186/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 2.3784 - val_accuracy: 0.9459\n",
            "Epoch 187/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0219 - accuracy: 0.9882 - val_loss: 2.3738 - val_accuracy: 0.9459\n",
            "Epoch 188/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 2.3463 - val_accuracy: 0.9459\n",
            "Epoch 189/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0152 - accuracy: 0.9983 - val_loss: 2.3523 - val_accuracy: 0.9459\n",
            "Epoch 190/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0156 - accuracy: 0.9915 - val_loss: 2.3764 - val_accuracy: 0.9459\n",
            "Epoch 191/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0204 - accuracy: 0.9915 - val_loss: 2.4130 - val_accuracy: 0.9459\n",
            "Epoch 192/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 2.4399 - val_accuracy: 0.9459\n",
            "Epoch 193/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0310 - accuracy: 0.9865 - val_loss: 2.4366 - val_accuracy: 0.9459\n",
            "Epoch 194/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 2.4393 - val_accuracy: 0.9459\n",
            "Epoch 195/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0117 - accuracy: 0.9949 - val_loss: 2.4435 - val_accuracy: 0.9459\n",
            "Epoch 196/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0245 - accuracy: 0.9932 - val_loss: 2.4427 - val_accuracy: 0.9459\n",
            "Epoch 197/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0138 - accuracy: 0.9949 - val_loss: 2.4259 - val_accuracy: 0.9459\n",
            "Epoch 198/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 2.4153 - val_accuracy: 0.9459\n",
            "Epoch 199/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0092 - accuracy: 0.9966 - val_loss: 2.4203 - val_accuracy: 0.9459\n",
            "Epoch 200/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 2.4267 - val_accuracy: 0.9459\n",
            "Epoch 201/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0138 - accuracy: 0.9949 - val_loss: 2.4269 - val_accuracy: 0.9459\n",
            "Epoch 202/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0142 - accuracy: 0.9932 - val_loss: 2.4372 - val_accuracy: 0.9459\n",
            "Epoch 203/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 2.4441 - val_accuracy: 0.9459\n",
            "Epoch 204/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 2.4483 - val_accuracy: 0.9459\n",
            "Epoch 205/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 2.5113 - val_accuracy: 0.9459\n",
            "Epoch 206/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 0.9915 - val_loss: 2.5264 - val_accuracy: 0.9459\n",
            "Epoch 207/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0130 - accuracy: 0.9932 - val_loss: 2.5132 - val_accuracy: 0.9459\n",
            "Epoch 208/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0232 - accuracy: 0.9898 - val_loss: 2.4264 - val_accuracy: 0.9459\n",
            "Epoch 209/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0088 - accuracy: 0.9983 - val_loss: 2.3808 - val_accuracy: 0.9459\n",
            "Epoch 210/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0159 - accuracy: 0.9932 - val_loss: 2.3910 - val_accuracy: 0.9459\n",
            "Epoch 211/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9949 - val_loss: 2.4116 - val_accuracy: 0.9459\n",
            "Epoch 212/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0184 - accuracy: 0.9915 - val_loss: 2.4447 - val_accuracy: 0.9459\n",
            "Epoch 213/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0435 - accuracy: 0.9848 - val_loss: 2.4404 - val_accuracy: 0.9459\n",
            "Epoch 214/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 0.9915 - val_loss: 2.4133 - val_accuracy: 0.9459\n",
            "Epoch 215/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0157 - accuracy: 0.9932 - val_loss: 2.3848 - val_accuracy: 0.9459\n",
            "Epoch 216/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0243 - accuracy: 0.9882 - val_loss: 2.3681 - val_accuracy: 0.9459\n",
            "Epoch 217/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0227 - accuracy: 0.9915 - val_loss: 2.3480 - val_accuracy: 0.9459\n",
            "Epoch 218/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0276 - accuracy: 0.9882 - val_loss: 2.3461 - val_accuracy: 0.9459\n",
            "Epoch 219/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 2.3551 - val_accuracy: 0.9459\n",
            "Epoch 220/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0181 - accuracy: 0.9898 - val_loss: 2.3824 - val_accuracy: 0.9459\n",
            "Epoch 221/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0182 - accuracy: 0.9915 - val_loss: 2.4539 - val_accuracy: 0.9459\n",
            "Epoch 222/2000\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 2.5206 - val_accuracy: 0.9459\n",
            "Epoch 223/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0193 - accuracy: 0.9915 - val_loss: 2.5542 - val_accuracy: 0.9459\n",
            "Epoch 224/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0195 - accuracy: 0.9932 - val_loss: 2.5572 - val_accuracy: 0.9459\n",
            "Epoch 225/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0293 - accuracy: 0.9848 - val_loss: 2.4261 - val_accuracy: 0.9459\n",
            "Epoch 226/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0132 - accuracy: 0.9932 - val_loss: 2.3469 - val_accuracy: 0.9459\n",
            "Epoch 227/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 2.3177 - val_accuracy: 0.9459\n",
            "Epoch 228/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 0.9932 - val_loss: 2.3179 - val_accuracy: 0.9459\n",
            "Epoch 229/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0212 - accuracy: 0.9915 - val_loss: 2.3248 - val_accuracy: 0.9459\n",
            "Epoch 230/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0133 - accuracy: 0.9949 - val_loss: 2.3003 - val_accuracy: 0.9459\n",
            "Epoch 231/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0140 - accuracy: 0.9915 - val_loss: 2.3176 - val_accuracy: 0.9459\n",
            "Epoch 232/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9898 - val_loss: 2.3677 - val_accuracy: 0.9459\n",
            "Epoch 233/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0206 - accuracy: 0.9915 - val_loss: 2.3019 - val_accuracy: 0.9459\n",
            "Epoch 234/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0121 - accuracy: 0.9949 - val_loss: 2.2514 - val_accuracy: 0.9459\n",
            "Epoch 235/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0211 - accuracy: 0.9882 - val_loss: 2.2278 - val_accuracy: 0.9459\n",
            "Epoch 236/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 2.2486 - val_accuracy: 0.9459\n",
            "Epoch 237/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0127 - accuracy: 0.9932 - val_loss: 2.2663 - val_accuracy: 0.9459\n",
            "Epoch 238/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0246 - accuracy: 0.9898 - val_loss: 2.2765 - val_accuracy: 0.9459\n",
            "Epoch 239/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0106 - accuracy: 0.9949 - val_loss: 2.2809 - val_accuracy: 0.9459\n",
            "Epoch 240/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 2.3190 - val_accuracy: 0.9459\n",
            "Epoch 241/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0293 - accuracy: 0.9882 - val_loss: 2.3244 - val_accuracy: 0.9459\n",
            "Epoch 242/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0110 - accuracy: 0.9949 - val_loss: 2.3158 - val_accuracy: 0.9459\n",
            "Epoch 243/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0238 - accuracy: 0.9848 - val_loss: 2.2960 - val_accuracy: 0.9459\n",
            "Epoch 244/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0211 - accuracy: 0.9915 - val_loss: 2.2697 - val_accuracy: 0.9459\n",
            "Epoch 245/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0257 - accuracy: 0.9882 - val_loss: 2.2392 - val_accuracy: 0.9459\n",
            "Epoch 246/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0278 - accuracy: 0.9848 - val_loss: 2.2504 - val_accuracy: 0.9459\n",
            "Epoch 247/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 2.2428 - val_accuracy: 0.9459\n",
            "Epoch 248/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 0.9949 - val_loss: 2.2555 - val_accuracy: 0.9459\n",
            "Epoch 249/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0170 - accuracy: 0.9915 - val_loss: 2.2738 - val_accuracy: 0.9459\n",
            "Epoch 250/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0087 - accuracy: 0.9949 - val_loss: 2.2924 - val_accuracy: 0.9459\n",
            "Epoch 251/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0235 - accuracy: 0.9865 - val_loss: 2.3126 - val_accuracy: 0.9459\n",
            "Epoch 252/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 2.3019 - val_accuracy: 0.9459\n",
            "Epoch 253/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0194 - accuracy: 0.9915 - val_loss: 2.2835 - val_accuracy: 0.9459\n",
            "Epoch 254/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 2.2820 - val_accuracy: 0.9459\n",
            "Epoch 255/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0334 - accuracy: 0.9898 - val_loss: 2.2663 - val_accuracy: 0.9459\n",
            "Epoch 256/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0200 - accuracy: 0.9865 - val_loss: 2.2837 - val_accuracy: 0.9459\n",
            "Epoch 257/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0219 - accuracy: 0.9932 - val_loss: 2.2721 - val_accuracy: 0.9459\n",
            "Epoch 258/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0361 - accuracy: 0.9831 - val_loss: 2.2790 - val_accuracy: 0.9459\n",
            "Epoch 259/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0230 - accuracy: 0.9898 - val_loss: 2.3155 - val_accuracy: 0.9459\n",
            "Epoch 260/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0148 - accuracy: 0.9966 - val_loss: 2.3489 - val_accuracy: 0.9459\n",
            "Epoch 261/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0230 - accuracy: 0.9915 - val_loss: 2.3298 - val_accuracy: 0.9459\n",
            "Epoch 262/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9932 - val_loss: 2.2723 - val_accuracy: 0.9459\n",
            "Epoch 263/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0364 - accuracy: 0.9882 - val_loss: 2.2306 - val_accuracy: 0.9459\n",
            "Epoch 264/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0193 - accuracy: 0.9915 - val_loss: 2.2481 - val_accuracy: 0.9459\n",
            "Epoch 265/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0201 - accuracy: 0.9915 - val_loss: 2.2748 - val_accuracy: 0.9459\n",
            "Epoch 266/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0182 - accuracy: 0.9915 - val_loss: 2.2911 - val_accuracy: 0.9459\n",
            "Epoch 267/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0122 - accuracy: 0.9949 - val_loss: 2.3142 - val_accuracy: 0.9459\n",
            "Epoch 268/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0208 - accuracy: 0.9898 - val_loss: 2.3612 - val_accuracy: 0.9459\n",
            "Epoch 269/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0163 - accuracy: 0.9932 - val_loss: 2.3848 - val_accuracy: 0.9459\n",
            "Epoch 270/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0213 - accuracy: 0.9949 - val_loss: 2.3983 - val_accuracy: 0.9459\n",
            "Epoch 271/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 0.9932 - val_loss: 2.3874 - val_accuracy: 0.9459\n",
            "Epoch 272/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0197 - accuracy: 0.9898 - val_loss: 2.3699 - val_accuracy: 0.9459\n",
            "Epoch 273/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0246 - accuracy: 0.9882 - val_loss: 2.2969 - val_accuracy: 0.9459\n",
            "Epoch 274/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 2.2553 - val_accuracy: 0.9459\n",
            "Epoch 275/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0116 - accuracy: 0.9949 - val_loss: 2.2315 - val_accuracy: 0.9459\n",
            "Epoch 276/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0200 - accuracy: 0.9915 - val_loss: 2.2243 - val_accuracy: 0.9459\n",
            "Epoch 277/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9932 - val_loss: 2.2173 - val_accuracy: 0.9459\n",
            "Epoch 278/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0176 - accuracy: 0.9915 - val_loss: 2.2768 - val_accuracy: 0.9459\n",
            "Epoch 279/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0141 - accuracy: 0.9949 - val_loss: 2.3462 - val_accuracy: 0.9459\n",
            "Epoch 280/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9915 - val_loss: 2.4404 - val_accuracy: 0.9459\n",
            "Epoch 281/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 0.9898 - val_loss: 2.5168 - val_accuracy: 0.9459\n",
            "Epoch 282/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 2.5109 - val_accuracy: 0.9459\n",
            "Epoch 283/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0191 - accuracy: 0.9898 - val_loss: 2.4217 - val_accuracy: 0.9459\n",
            "Epoch 284/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0103 - accuracy: 0.9949 - val_loss: 2.3556 - val_accuracy: 0.9459\n",
            "Epoch 285/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0291 - accuracy: 0.9882 - val_loss: 2.2482 - val_accuracy: 0.9459\n",
            "Epoch 286/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0152 - accuracy: 0.9932 - val_loss: 2.1855 - val_accuracy: 0.9459\n",
            "Epoch 287/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 0.9932 - val_loss: 2.1778 - val_accuracy: 0.9459\n",
            "Epoch 288/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0188 - accuracy: 0.9915 - val_loss: 2.1788 - val_accuracy: 0.9459\n",
            "Epoch 289/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 0.9932 - val_loss: 2.1833 - val_accuracy: 0.9459\n",
            "Epoch 290/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.2060 - val_accuracy: 0.9459\n",
            "Epoch 291/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0120 - accuracy: 0.9949 - val_loss: 2.2492 - val_accuracy: 0.9459\n",
            "Epoch 292/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0121 - accuracy: 0.9949 - val_loss: 2.2782 - val_accuracy: 0.9459\n",
            "Epoch 293/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0266 - accuracy: 0.9898 - val_loss: 2.3657 - val_accuracy: 0.9459\n",
            "Epoch 294/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 2.5274 - val_accuracy: 0.9459\n",
            "Epoch 295/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0242 - accuracy: 0.9915 - val_loss: 2.6164 - val_accuracy: 0.9459\n",
            "Epoch 296/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 2.4583 - val_accuracy: 0.9459\n",
            "Epoch 297/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0132 - accuracy: 0.9949 - val_loss: 2.2365 - val_accuracy: 0.9459\n",
            "Epoch 298/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0247 - accuracy: 0.9865 - val_loss: 2.1473 - val_accuracy: 0.9459\n",
            "Epoch 299/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0314 - accuracy: 0.9848 - val_loss: 2.2181 - val_accuracy: 0.9459\n",
            "Epoch 300/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0271 - accuracy: 0.9915 - val_loss: 2.4188 - val_accuracy: 0.9459\n",
            "Epoch 301/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9915 - val_loss: 2.5011 - val_accuracy: 0.9459\n",
            "Epoch 302/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 2.5147 - val_accuracy: 0.9459\n",
            "Epoch 303/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0179 - accuracy: 0.9932 - val_loss: 2.4521 - val_accuracy: 0.9459\n",
            "Epoch 304/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0235 - accuracy: 0.9915 - val_loss: 2.4150 - val_accuracy: 0.9459\n",
            "Epoch 305/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0232 - accuracy: 0.9898 - val_loss: 2.4666 - val_accuracy: 0.9459\n",
            "Epoch 306/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0151 - accuracy: 0.9932 - val_loss: 2.5244 - val_accuracy: 0.9459\n",
            "Epoch 307/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0169 - accuracy: 0.9949 - val_loss: 2.5594 - val_accuracy: 0.9459\n",
            "Epoch 308/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0149 - accuracy: 0.9932 - val_loss: 2.5871 - val_accuracy: 0.9459\n",
            "Epoch 309/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 2.6033 - val_accuracy: 0.9459\n",
            "Epoch 310/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 2.5991 - val_accuracy: 0.9459\n",
            "Epoch 311/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0205 - accuracy: 0.9915 - val_loss: 2.6025 - val_accuracy: 0.9459\n",
            "Epoch 312/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0112 - accuracy: 0.9932 - val_loss: 2.6311 - val_accuracy: 0.9459\n",
            "Epoch 313/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0165 - accuracy: 0.9932 - val_loss: 2.6358 - val_accuracy: 0.9459\n",
            "Epoch 314/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 2.6317 - val_accuracy: 0.9459\n",
            "Epoch 315/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0360 - accuracy: 0.9848 - val_loss: 2.6262 - val_accuracy: 0.9459\n",
            "Epoch 316/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0242 - accuracy: 0.9865 - val_loss: 2.5662 - val_accuracy: 0.9459\n",
            "Epoch 317/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0138 - accuracy: 0.9949 - val_loss: 2.5491 - val_accuracy: 0.9459\n",
            "Epoch 318/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.9932 - val_loss: 2.5469 - val_accuracy: 0.9459\n",
            "Epoch 319/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0211 - accuracy: 0.9915 - val_loss: 2.5579 - val_accuracy: 0.9459\n",
            "Epoch 320/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0205 - accuracy: 0.9898 - val_loss: 2.5204 - val_accuracy: 0.9459\n",
            "Epoch 321/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0158 - accuracy: 0.9915 - val_loss: 2.4926 - val_accuracy: 0.9459\n",
            "Epoch 322/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 2.5188 - val_accuracy: 0.9459\n",
            "Epoch 323/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0151 - accuracy: 0.9915 - val_loss: 2.5644 - val_accuracy: 0.9459\n",
            "Epoch 324/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0276 - accuracy: 0.9848 - val_loss: 2.5896 - val_accuracy: 0.9459\n",
            "Epoch 325/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0178 - accuracy: 0.9932 - val_loss: 2.6016 - val_accuracy: 0.9459\n",
            "Epoch 326/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0132 - accuracy: 0.9932 - val_loss: 2.5841 - val_accuracy: 0.9459\n",
            "Epoch 327/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9915 - val_loss: 2.5896 - val_accuracy: 0.9459\n",
            "Epoch 328/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0230 - accuracy: 0.9915 - val_loss: 2.5921 - val_accuracy: 0.9459\n",
            "Epoch 329/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0177 - accuracy: 0.9915 - val_loss: 2.6064 - val_accuracy: 0.9459\n",
            "Epoch 330/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0160 - accuracy: 0.9915 - val_loss: 2.5965 - val_accuracy: 0.9459\n",
            "Epoch 331/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0154 - accuracy: 0.9932 - val_loss: 2.5995 - val_accuracy: 0.9459\n",
            "Epoch 332/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0163 - accuracy: 0.9898 - val_loss: 2.6533 - val_accuracy: 0.9459\n",
            "Epoch 333/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0186 - accuracy: 0.9915 - val_loss: 2.6875 - val_accuracy: 0.9459\n",
            "Epoch 334/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0340 - accuracy: 0.9915 - val_loss: 2.6671 - val_accuracy: 0.9459\n",
            "Epoch 335/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0137 - accuracy: 0.9932 - val_loss: 2.6033 - val_accuracy: 0.9459\n",
            "Epoch 336/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0156 - accuracy: 0.9898 - val_loss: 2.5770 - val_accuracy: 0.9459\n",
            "Epoch 337/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0211 - accuracy: 0.9915 - val_loss: 2.5751 - val_accuracy: 0.9459\n",
            "Epoch 338/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0294 - accuracy: 0.9898 - val_loss: 2.5597 - val_accuracy: 0.9459\n",
            "Epoch 339/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0259 - accuracy: 0.9932 - val_loss: 2.4880 - val_accuracy: 0.9459\n",
            "Epoch 340/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 0.9932 - val_loss: 2.4808 - val_accuracy: 0.9459\n",
            "Epoch 341/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0246 - accuracy: 0.9915 - val_loss: 2.5274 - val_accuracy: 0.9459\n",
            "Epoch 342/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0202 - accuracy: 0.9882 - val_loss: 2.5832 - val_accuracy: 0.9459\n",
            "Epoch 343/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0254 - accuracy: 0.9814 - val_loss: 2.5984 - val_accuracy: 0.9459\n",
            "Epoch 344/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9932 - val_loss: 2.6263 - val_accuracy: 0.9459\n",
            "Epoch 345/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0175 - accuracy: 0.9848 - val_loss: 2.6233 - val_accuracy: 0.9459\n",
            "Epoch 346/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 2.6066 - val_accuracy: 0.9459\n",
            "Epoch 347/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0123 - accuracy: 0.9932 - val_loss: 2.6034 - val_accuracy: 0.9459\n",
            "Epoch 348/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0213 - accuracy: 0.9915 - val_loss: 2.6266 - val_accuracy: 0.9459\n",
            "Epoch 349/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0370 - accuracy: 0.9966 - val_loss: 2.6476 - val_accuracy: 0.9459\n",
            "Epoch 350/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0120 - accuracy: 0.9949 - val_loss: 2.6409 - val_accuracy: 0.9459\n",
            "Epoch 351/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0235 - accuracy: 0.9882 - val_loss: 2.6597 - val_accuracy: 0.9459\n",
            "Epoch 352/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0328 - accuracy: 0.9915 - val_loss: 2.6019 - val_accuracy: 0.9459\n",
            "Epoch 353/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0162 - accuracy: 0.9915 - val_loss: 2.6148 - val_accuracy: 0.9459\n",
            "Epoch 354/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0254 - accuracy: 0.9949 - val_loss: 2.6601 - val_accuracy: 0.9459\n",
            "Epoch 355/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0222 - accuracy: 0.9882 - val_loss: 2.6893 - val_accuracy: 0.9459\n",
            "Epoch 356/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0160 - accuracy: 0.9915 - val_loss: 2.7022 - val_accuracy: 0.9459\n",
            "Epoch 357/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0313 - accuracy: 0.9932 - val_loss: 2.6957 - val_accuracy: 0.9459\n",
            "Epoch 358/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 2.6957 - val_accuracy: 0.9459\n",
            "Epoch 359/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9932 - val_loss: 2.6995 - val_accuracy: 0.9459\n",
            "Epoch 360/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0162 - accuracy: 0.9966 - val_loss: 2.6969 - val_accuracy: 0.9459\n",
            "Epoch 361/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0186 - accuracy: 0.9932 - val_loss: 2.6461 - val_accuracy: 0.9459\n",
            "Epoch 362/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0248 - accuracy: 0.9932 - val_loss: 2.6823 - val_accuracy: 0.9459\n",
            "Epoch 363/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0193 - accuracy: 0.9898 - val_loss: 2.7428 - val_accuracy: 0.9459\n",
            "Epoch 364/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0159 - accuracy: 0.9882 - val_loss: 2.8052 - val_accuracy: 0.9459\n",
            "Epoch 365/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 2.8418 - val_accuracy: 0.9459\n",
            "Epoch 366/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0220 - accuracy: 0.9932 - val_loss: 2.8316 - val_accuracy: 0.9459\n",
            "Epoch 367/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 2.8210 - val_accuracy: 0.9459\n",
            "Epoch 368/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 0.9932 - val_loss: 2.8101 - val_accuracy: 0.9459\n",
            "Epoch 369/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 2.7875 - val_accuracy: 0.9459\n",
            "Epoch 370/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 2.7911 - val_accuracy: 0.9459\n",
            "Epoch 371/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 2.7931 - val_accuracy: 0.9459\n",
            "Epoch 372/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0113 - accuracy: 0.9949 - val_loss: 2.8047 - val_accuracy: 0.9459\n",
            "Epoch 373/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0149 - accuracy: 0.9932 - val_loss: 2.8404 - val_accuracy: 0.9459\n",
            "Epoch 374/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 2.9096 - val_accuracy: 0.9459\n",
            "Epoch 375/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0167 - accuracy: 0.9898 - val_loss: 2.9638 - val_accuracy: 0.9459\n",
            "Epoch 376/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0180 - accuracy: 0.9932 - val_loss: 3.0214 - val_accuracy: 0.9459\n",
            "Epoch 377/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 0.9949 - val_loss: 3.0569 - val_accuracy: 0.9459\n",
            "Epoch 378/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0178 - accuracy: 0.9915 - val_loss: 3.0694 - val_accuracy: 0.9459\n",
            "Epoch 379/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 2.9488 - val_accuracy: 0.9459\n",
            "Epoch 380/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 2.8642 - val_accuracy: 0.9459\n",
            "Epoch 381/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0103 - accuracy: 0.9949 - val_loss: 2.8323 - val_accuracy: 0.9459\n",
            "Epoch 382/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0181 - accuracy: 0.9915 - val_loss: 2.8291 - val_accuracy: 0.9459\n",
            "Epoch 383/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0163 - accuracy: 0.9932 - val_loss: 2.8671 - val_accuracy: 0.9459\n",
            "Epoch 384/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.9198 - val_accuracy: 0.9459\n",
            "Epoch 385/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0137 - accuracy: 0.9915 - val_loss: 2.9858 - val_accuracy: 0.9459\n",
            "Epoch 386/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0202 - accuracy: 0.9915 - val_loss: 3.0324 - val_accuracy: 0.9459\n",
            "Epoch 387/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0121 - accuracy: 0.9949 - val_loss: 3.0290 - val_accuracy: 0.9459\n",
            "Epoch 388/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0179 - accuracy: 0.9932 - val_loss: 2.9537 - val_accuracy: 0.9459\n",
            "Epoch 389/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 2.8961 - val_accuracy: 0.9459\n",
            "Epoch 390/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0253 - accuracy: 0.9966 - val_loss: 2.8784 - val_accuracy: 0.9459\n",
            "Epoch 391/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 2.8847 - val_accuracy: 0.9459\n",
            "Epoch 392/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0245 - accuracy: 0.9898 - val_loss: 2.9139 - val_accuracy: 0.9459\n",
            "Epoch 393/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 2.8955 - val_accuracy: 0.9459\n",
            "Epoch 394/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0179 - accuracy: 0.9932 - val_loss: 2.8825 - val_accuracy: 0.9459\n",
            "Epoch 395/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0278 - accuracy: 0.9865 - val_loss: 2.8375 - val_accuracy: 0.9459\n",
            "Epoch 396/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 2.8147 - val_accuracy: 0.9459\n",
            "Epoch 397/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0112 - accuracy: 0.9949 - val_loss: 2.8195 - val_accuracy: 0.9459\n",
            "Epoch 398/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 0.9966 - val_loss: 2.8543 - val_accuracy: 0.9459\n",
            "Epoch 399/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 0.9932 - val_loss: 2.9088 - val_accuracy: 0.9459\n",
            "Epoch 400/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 2.9600 - val_accuracy: 0.9459\n",
            "Epoch 401/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 0.9915 - val_loss: 3.0194 - val_accuracy: 0.9459\n",
            "Epoch 402/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0196 - accuracy: 0.9882 - val_loss: 3.0205 - val_accuracy: 0.9459\n",
            "Epoch 403/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0219 - accuracy: 0.9898 - val_loss: 3.0092 - val_accuracy: 0.9459\n",
            "Epoch 404/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0141 - accuracy: 0.9915 - val_loss: 2.9859 - val_accuracy: 0.9459\n",
            "Epoch 405/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0146 - accuracy: 0.9915 - val_loss: 2.9272 - val_accuracy: 0.9459\n",
            "Epoch 406/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9932 - val_loss: 2.9211 - val_accuracy: 0.9459\n",
            "Epoch 407/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0251 - accuracy: 0.9932 - val_loss: 2.8809 - val_accuracy: 0.9459\n",
            "Epoch 408/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 2.8285 - val_accuracy: 0.9459\n",
            "Epoch 409/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0175 - accuracy: 0.9915 - val_loss: 2.8339 - val_accuracy: 0.9459\n",
            "Epoch 410/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0177 - accuracy: 0.9915 - val_loss: 2.8111 - val_accuracy: 0.9459\n",
            "Epoch 411/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0139 - accuracy: 0.9932 - val_loss: 2.8013 - val_accuracy: 0.9459\n",
            "Epoch 412/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0152 - accuracy: 0.9932 - val_loss: 2.8212 - val_accuracy: 0.9459\n",
            "Epoch 413/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0127 - accuracy: 0.9949 - val_loss: 2.8684 - val_accuracy: 0.9459\n",
            "Epoch 414/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 0.9949 - val_loss: 2.8792 - val_accuracy: 0.9459\n",
            "Epoch 415/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0144 - accuracy: 0.9949 - val_loss: 2.8956 - val_accuracy: 0.9459\n",
            "Epoch 416/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 0.9949 - val_loss: 2.9211 - val_accuracy: 0.9459\n",
            "Epoch 417/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0163 - accuracy: 0.9932 - val_loss: 2.9671 - val_accuracy: 0.9459\n",
            "Epoch 418/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 3.0019 - val_accuracy: 0.9459\n",
            "Epoch 419/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0123 - accuracy: 0.9949 - val_loss: 3.0363 - val_accuracy: 0.9459\n",
            "Epoch 420/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0233 - accuracy: 0.9898 - val_loss: 3.0507 - val_accuracy: 0.9459\n",
            "Epoch 421/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 0.9949 - val_loss: 3.0700 - val_accuracy: 0.9459\n",
            "Epoch 422/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 2.9847 - val_accuracy: 0.9459\n",
            "Epoch 423/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0134 - accuracy: 0.9915 - val_loss: 2.9467 - val_accuracy: 0.9459\n",
            "Epoch 424/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 2.9409 - val_accuracy: 0.9459\n",
            "Epoch 425/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 0.9915 - val_loss: 2.9496 - val_accuracy: 0.9459\n",
            "Epoch 426/2000\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0176 - accuracy: 0.9915 - val_loss: 2.9365 - val_accuracy: 0.9459\n",
            "Epoch 427/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0221 - accuracy: 0.9915 - val_loss: 2.8541 - val_accuracy: 0.9459\n",
            "Epoch 428/2000\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0141 - accuracy: 0.9932 - val_loss: 2.8046 - val_accuracy: 0.9459\n",
            "Epoch 429/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 2.7821 - val_accuracy: 0.9459\n",
            "Epoch 430/2000\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0544 - accuracy: 0.9949 - val_loss: 2.7336 - val_accuracy: 0.9459\n",
            "Epoch 431/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 2.7284 - val_accuracy: 0.9459\n",
            "Epoch 432/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 2.7663 - val_accuracy: 0.9459\n",
            "Epoch 433/2000\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0141 - accuracy: 0.9932 - val_loss: 2.8545 - val_accuracy: 0.9459\n",
            "Epoch 434/2000\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0185 - accuracy: 0.9932 - val_loss: 2.8469 - val_accuracy: 0.9459\n",
            "Epoch 435/2000\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 2.8589 - val_accuracy: 0.9459\n"
          ]
        }
      ],
      "source": [
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=400, restore_best_weights=True)]\n",
        "\n",
        "history=model.fit(\n",
        "    [x_train1,x_train],\n",
        "    y_train1,\n",
        "    validation_data=([x_test1, x_test], y_test1),\n",
        "    epochs=2000,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing prediction\n",
        "\n",
        "#testing prediction\n",
        "ypred1=model.predict([x_test1, x_test])\n",
        "\n",
        "ypred=[]\n",
        "#testing prediction\n",
        "for i in ypred1:\n",
        "    if i[0]>i[1]:\n",
        "        ypred.append(0)\n",
        "    else:\n",
        "        ypred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print(accuracy_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(ypred, y_test1))\n",
        "print('roc_auc_score', roc_auc_score(ypred, y_test1))\n",
        "print('precision_score', precision_score(ypred, y_test1))\n",
        "\n",
        "print('recall_score', recall_score(ypred, y_test1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HPQnjhbbPGD",
        "outputId": "887e26bf-a38b-4326-bbbd-3893880567db"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9459459459459459\n",
            "f1_score 0.943661971830986\n",
            "jaccard_score 0.8933333333333333\n",
            "roc_auc_score 0.9465201465201466\n",
            "precision_score 0.9305555555555556\n",
            "recall_score 0.9571428571428572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training prediction\n",
        "xpred1=model.predict([x_train1, x_train])\n",
        "xpred=[]\n",
        "\n",
        "for i in xpred1:\n",
        "    if i[0]>i[1]:\n",
        "        xpred.append(0)\n",
        "    else:\n",
        "        xpred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(xpred, y_train1))\n",
        "print('roc_auc_score', roc_auc_score(xpred, y_train1))\n",
        "print('precision_score', precision_score(xpred, y_train1))\n",
        "\n",
        "print('recall_score', recall_score(xpred, y_train1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdq3NiNkidTO",
        "outputId": "28b81893-8452-405e-b5d0-ad2d17900841"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 1.0\n",
            "f1_score 1.0\n",
            "jaccard_score 1.0\n",
            "roc_auc_score 1.0\n",
            "precision_score 1.0\n",
            "recall_score 1.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rct_Absenteeism_at_work.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}