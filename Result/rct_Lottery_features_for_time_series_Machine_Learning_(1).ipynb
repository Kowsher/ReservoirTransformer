{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M2icok5AYzvS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/univ-length-over10.short-col-names.csv')\n",
        "\n",
        "#df=df.drop(columns = ['date'])\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "#df = imputer.fit_transform(df)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1= LabelEncoder()\n",
        "le2= LabelEncoder()\n",
        "le3= LabelEncoder()\n",
        "df['predWillFollowIncreaseCapacity']=le1.fit_transform(df['predWillFollowIncreaseCapacity'])\n",
        "df['Ft-ULen-Over10-didFollowIncreaseCapacity']=le2.fit_transform(df['Ft-ULen-Over10-didFollowIncreaseCapacity'])\n",
        "\n",
        "'''\n",
        "df=df.dropna()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "'''\n",
        "\n",
        "\n",
        "#df['icon']= le.fit_transform(df['icon'])\n",
        "\n",
        "y = df['Ft-ULen-Over10-didFollowIncreaseCapacity'].values\n",
        "df=df.drop(columns = ['Ft-ULen-Over10-didFollowIncreaseCapacity'])\n",
        "X = df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_82uIQCFdziZ",
        "outputId": "911ce513-ab9b-4683-ee29-615d3128ebb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 359, 1: 235})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=X[1:]\n",
        "y=y[1:]"
      ],
      "metadata": {
        "id": "Mx_UvqUVAPEj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y)):\n",
        "    if y[i]==2:\n",
        "        y[i]=1"
      ],
      "metadata": {
        "id": "0zywkkq4f_e3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "UPKOzyqdgJxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Rn2VoK1owI5"
      },
      "outputs": [],
      "source": [
        "\n",
        "y=y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X=np.concatenate((X[1:], y[0:-1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxWNccXgu12l",
        "outputId": "ab9dcf70-6fda-46c6-f306-aee9848350e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=87267e5d0e697dca9a1aedad98cd877492fb44962202b633ea0393fae99b28cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8iIZ1105tFnf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X= sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl0SFJ5WtbBP",
        "outputId": "e9689ab1-a722-4cad-b0bf-8a1784aefa14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrcn\n",
            "  Downloading PyRCN-0.0.16.post1-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81 kB 6.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyrcn\n",
            "Successfully installed pyrcn-0.0.16.post1\n"
          ]
        }
      ],
      "source": [
        "pip install pyrcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gEPWGptbuTYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y[1:], test_size=0.2, shuffle=False, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDAxxhLupYK6",
        "outputId": "aae297f7-5e78-47d2-f859-ee03efeb8f3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(473, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1rOeQ0-ptl_G"
      },
      "outputs": [],
      "source": [
        "from pyrcn.base.blocks import InputToNode\n",
        "from sklearn . datasets import make_blobs\n",
        "# Generate a toy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tShY5ktnrN",
        "outputId": "8204e60c-7e7a-4c44-9f85-65b1e6ed2b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=100 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "input_to_node = InputToNode (100, input_activation='relu',input_scaling =1.0 )\n",
        "\n",
        "\n",
        "x_train= input_to_node.fit_transform (x_train1)\n",
        "x_test= input_to_node.transform (x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDw3vyTRuB-o",
        "outputId": "1aa7a23a-75f1-48ed-aa22-4fe7dd03f9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=100 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from pyrcn.base.blocks import NodeToNode\n",
        "node_to_node = NodeToNode (100, reservoir_activation='relu', spectral_radius =1.0 , leakage =0.5 ,bidirectional = False )\n",
        "x_train=node_to_node . fit_transform(x_train)\n",
        "x_test= node_to_node.transform (x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b-dqEbXtuCpY"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "x_train1 = x_train1.reshape((x_train1.shape[0], x_train1.shape[1], 1))\n",
        "x_test1 = x_test1.reshape((x_test1.shape[0], x_test1.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qi0MhVmsuswU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NX3w9IqBuuZ-"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HsU6bO-PuvqD"
      },
      "outputs": [],
      "source": [
        "from keras_self_attention import SeqSelfAttention\n",
        "def build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputsy = keras.Input(shape=input_shapey)\n",
        "    inputsz = keras.Input(shape=input_shapez)\n",
        "    y = inputsy\n",
        "    z= inputsz\n",
        "\n",
        "    #z=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(z)\n",
        "\n",
        "    #y=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(y)\n",
        "\n",
        "    z=SeqSelfAttention()(z)\n",
        "\n",
        "    #y=SeqSelfAttention()(y)\n",
        "\n",
        "\n",
        "\n",
        "    x=layers.Concatenate(axis=1)([y, z])\n",
        "    #x=layers.Add()([inputsy, inputsz])\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    #outputs = layers.Dense(1)(x)\n",
        "    return keras.Model([inputsy,inputsz], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Hu3CH4vZzu",
        "outputId": "f176f6eb-07cb-4da9-eee0-8ed63171d878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 20, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " seq_self_attention (SeqSelfAtt  (None, 100, 1)      129         ['input_2[0][0]']                \n",
            " ention)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 120, 1)       0           ['input_1[0][0]',                \n",
            "                                                                  'seq_self_attention[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 120, 1)      2           ['concatenate[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 120, 1)      7169        ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 120, 1)       0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 120, 1)      0           ['dropout[0][0]',                \n",
            " da)                                                              'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 120, 4)       8           ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 120, 4)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 120, 1)       5           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 120, 1)      0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 120, 1)      7169        ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 120, 1)       0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 120, 1)      0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 120, 4)       8           ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 120, 4)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 120, 1)       5           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 120, 1)      0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 120, 1)      7169        ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 120, 1)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 120, 1)      0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 120, 4)       8           ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 120, 4)       0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 120, 1)       5           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 120, 1)      0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 120, 1)      7169        ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 120, 1)       0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 120, 1)      0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 120, 1)      2           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 120, 4)       8           ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 120, 4)       0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 120, 1)       5           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 120, 1)      0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 120)         0           ['tf.__operators__.add_7[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          15488       ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            258         ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 44,619\n",
            "Trainable params: 44,619\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shapey = x_train1.shape[1:]\n",
        "input_shapez = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.7,\n",
        "    dropout=0.75,\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"mean_absolute_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"mean_absolute_error\"],\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jta9P_Q3vkrc",
        "outputId": "7152360c-63cf-4f61-8fa2-3093922dccd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 6s 307ms/step - loss: 1.6532 - accuracy: 0.5391 - val_loss: 0.5788 - val_accuracy: 0.6891\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.9470 - accuracy: 0.5941 - val_loss: 0.6247 - val_accuracy: 0.6639\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.7489 - accuracy: 0.6744 - val_loss: 0.7499 - val_accuracy: 0.5714\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.6842 - accuracy: 0.6681 - val_loss: 0.5657 - val_accuracy: 0.6891\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.6009 - accuracy: 0.7146 - val_loss: 0.5792 - val_accuracy: 0.7479\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.5816 - accuracy: 0.7230 - val_loss: 0.5736 - val_accuracy: 0.7059\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 107ms/step - loss: 0.5784 - accuracy: 0.6956 - val_loss: 0.5463 - val_accuracy: 0.7311\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.5621 - accuracy: 0.7505 - val_loss: 0.5376 - val_accuracy: 0.7563\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.5743 - accuracy: 0.7188 - val_loss: 0.5394 - val_accuracy: 0.7143\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 0.5430 - accuracy: 0.7336 - val_loss: 0.5228 - val_accuracy: 0.7563\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.5492 - accuracy: 0.7463 - val_loss: 0.5287 - val_accuracy: 0.7395\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 0.5264 - accuracy: 0.7400 - val_loss: 0.5147 - val_accuracy: 0.7647\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 0.5373 - accuracy: 0.7717 - val_loss: 0.5121 - val_accuracy: 0.7563\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 0.5076 - accuracy: 0.7484 - val_loss: 0.5103 - val_accuracy: 0.7479\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 115ms/step - loss: 0.5091 - accuracy: 0.7548 - val_loss: 0.5023 - val_accuracy: 0.7479\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.4884 - accuracy: 0.7886 - val_loss: 0.4842 - val_accuracy: 0.7563\n",
            "Epoch 17/1000\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 0.4857 - accuracy: 0.7844 - val_loss: 0.4790 - val_accuracy: 0.7647\n",
            "Epoch 18/1000\n",
            "4/4 [==============================] - 0s 117ms/step - loss: 0.4795 - accuracy: 0.8076 - val_loss: 0.4732 - val_accuracy: 0.7815\n",
            "Epoch 19/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.4780 - accuracy: 0.7928 - val_loss: 0.4751 - val_accuracy: 0.7731\n",
            "Epoch 20/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.4512 - accuracy: 0.7928 - val_loss: 0.4615 - val_accuracy: 0.7731\n",
            "Epoch 21/1000\n",
            "4/4 [==============================] - 0s 108ms/step - loss: 0.4436 - accuracy: 0.7886 - val_loss: 0.4538 - val_accuracy: 0.7815\n",
            "Epoch 22/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.4635 - accuracy: 0.8034 - val_loss: 0.4598 - val_accuracy: 0.7647\n",
            "Epoch 23/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.4424 - accuracy: 0.7992 - val_loss: 0.4519 - val_accuracy: 0.7563\n",
            "Epoch 24/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.4404 - accuracy: 0.7970 - val_loss: 0.4537 - val_accuracy: 0.7731\n",
            "Epoch 25/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.4636 - accuracy: 0.8013 - val_loss: 0.4682 - val_accuracy: 0.7899\n",
            "Epoch 26/1000\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 0.4238 - accuracy: 0.8118 - val_loss: 0.4595 - val_accuracy: 0.7731\n",
            "Epoch 27/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.4107 - accuracy: 0.8288 - val_loss: 0.4349 - val_accuracy: 0.7899\n",
            "Epoch 28/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.4169 - accuracy: 0.8161 - val_loss: 0.4611 - val_accuracy: 0.7815\n",
            "Epoch 29/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.4082 - accuracy: 0.8034 - val_loss: 0.4154 - val_accuracy: 0.8067\n",
            "Epoch 30/1000\n",
            "4/4 [==============================] - 0s 106ms/step - loss: 0.4077 - accuracy: 0.8140 - val_loss: 0.4286 - val_accuracy: 0.7647\n",
            "Epoch 31/1000\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 0.4067 - accuracy: 0.8288 - val_loss: 0.4181 - val_accuracy: 0.7815\n",
            "Epoch 32/1000\n",
            "4/4 [==============================] - 0s 116ms/step - loss: 0.3945 - accuracy: 0.8393 - val_loss: 0.4065 - val_accuracy: 0.7983\n",
            "Epoch 33/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3863 - accuracy: 0.8182 - val_loss: 0.4288 - val_accuracy: 0.7983\n",
            "Epoch 34/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3717 - accuracy: 0.8203 - val_loss: 0.4151 - val_accuracy: 0.7899\n",
            "Epoch 35/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3713 - accuracy: 0.8393 - val_loss: 0.4299 - val_accuracy: 0.7899\n",
            "Epoch 36/1000\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 0.3962 - accuracy: 0.8097 - val_loss: 0.4040 - val_accuracy: 0.8067\n",
            "Epoch 37/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.3917 - accuracy: 0.8330 - val_loss: 0.3998 - val_accuracy: 0.7983\n",
            "Epoch 38/1000\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 0.3717 - accuracy: 0.8309 - val_loss: 0.3900 - val_accuracy: 0.7983\n",
            "Epoch 39/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.3884 - accuracy: 0.8351 - val_loss: 0.3955 - val_accuracy: 0.7899\n",
            "Epoch 40/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.3685 - accuracy: 0.8499 - val_loss: 0.3862 - val_accuracy: 0.7899\n",
            "Epoch 41/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.3559 - accuracy: 0.8309 - val_loss: 0.3875 - val_accuracy: 0.7899\n",
            "Epoch 42/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3619 - accuracy: 0.8330 - val_loss: 0.3873 - val_accuracy: 0.7815\n",
            "Epoch 43/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3711 - accuracy: 0.8309 - val_loss: 0.3866 - val_accuracy: 0.7899\n",
            "Epoch 44/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.3363 - accuracy: 0.8436 - val_loss: 0.3720 - val_accuracy: 0.8067\n",
            "Epoch 45/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3520 - accuracy: 0.8393 - val_loss: 0.3770 - val_accuracy: 0.7983\n",
            "Epoch 46/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 0.3465 - accuracy: 0.8457 - val_loss: 0.3699 - val_accuracy: 0.8067\n",
            "Epoch 47/1000\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 0.3363 - accuracy: 0.8541 - val_loss: 0.3620 - val_accuracy: 0.8403\n",
            "Epoch 48/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3288 - accuracy: 0.8478 - val_loss: 0.3735 - val_accuracy: 0.8067\n",
            "Epoch 49/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3292 - accuracy: 0.8436 - val_loss: 0.3778 - val_accuracy: 0.8067\n",
            "Epoch 50/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3385 - accuracy: 0.8499 - val_loss: 0.3742 - val_accuracy: 0.7899\n",
            "Epoch 51/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.3366 - accuracy: 0.8330 - val_loss: 0.3763 - val_accuracy: 0.7899\n",
            "Epoch 52/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3024 - accuracy: 0.8541 - val_loss: 0.3728 - val_accuracy: 0.7983\n",
            "Epoch 53/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3467 - accuracy: 0.8436 - val_loss: 0.3720 - val_accuracy: 0.8067\n",
            "Epoch 54/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3173 - accuracy: 0.8732 - val_loss: 0.3808 - val_accuracy: 0.7983\n",
            "Epoch 55/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3157 - accuracy: 0.8668 - val_loss: 0.3702 - val_accuracy: 0.7899\n",
            "Epoch 56/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.3069 - accuracy: 0.8584 - val_loss: 0.3789 - val_accuracy: 0.7899\n",
            "Epoch 57/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3004 - accuracy: 0.8626 - val_loss: 0.3813 - val_accuracy: 0.7983\n",
            "Epoch 58/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2985 - accuracy: 0.8668 - val_loss: 0.3751 - val_accuracy: 0.7899\n",
            "Epoch 59/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2902 - accuracy: 0.8710 - val_loss: 0.3715 - val_accuracy: 0.7983\n",
            "Epoch 60/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2958 - accuracy: 0.8562 - val_loss: 0.3642 - val_accuracy: 0.8403\n",
            "Epoch 61/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3285 - accuracy: 0.8457 - val_loss: 0.3890 - val_accuracy: 0.8067\n",
            "Epoch 62/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2937 - accuracy: 0.8647 - val_loss: 0.3764 - val_accuracy: 0.8235\n",
            "Epoch 63/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2972 - accuracy: 0.8689 - val_loss: 0.4001 - val_accuracy: 0.8067\n",
            "Epoch 64/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3190 - accuracy: 0.8520 - val_loss: 0.3767 - val_accuracy: 0.8571\n",
            "Epoch 65/1000\n",
            "4/4 [==============================] - 0s 113ms/step - loss: 0.3120 - accuracy: 0.8414 - val_loss: 0.3815 - val_accuracy: 0.8403\n",
            "Epoch 66/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.3404 - accuracy: 0.8499 - val_loss: 0.3932 - val_accuracy: 0.8067\n",
            "Epoch 67/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3447 - accuracy: 0.8457 - val_loss: 0.3900 - val_accuracy: 0.7899\n",
            "Epoch 68/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.3092 - accuracy: 0.8541 - val_loss: 0.3822 - val_accuracy: 0.8067\n",
            "Epoch 69/1000\n",
            "4/4 [==============================] - 0s 110ms/step - loss: 0.3162 - accuracy: 0.8393 - val_loss: 0.3598 - val_accuracy: 0.8403\n",
            "Epoch 70/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3136 - accuracy: 0.8541 - val_loss: 0.3683 - val_accuracy: 0.8319\n",
            "Epoch 71/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3070 - accuracy: 0.8626 - val_loss: 0.3737 - val_accuracy: 0.8319\n",
            "Epoch 72/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.3179 - accuracy: 0.8562 - val_loss: 0.3972 - val_accuracy: 0.8487\n",
            "Epoch 73/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.3081 - accuracy: 0.8520 - val_loss: 0.3882 - val_accuracy: 0.8403\n",
            "Epoch 74/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2824 - accuracy: 0.8774 - val_loss: 0.3799 - val_accuracy: 0.8235\n",
            "Epoch 75/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2995 - accuracy: 0.8647 - val_loss: 0.3811 - val_accuracy: 0.7983\n",
            "Epoch 76/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.2804 - accuracy: 0.8816 - val_loss: 0.3706 - val_accuracy: 0.8067\n",
            "Epoch 77/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2779 - accuracy: 0.8710 - val_loss: 0.3952 - val_accuracy: 0.8151\n",
            "Epoch 78/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2773 - accuracy: 0.8689 - val_loss: 0.3832 - val_accuracy: 0.8571\n",
            "Epoch 79/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2592 - accuracy: 0.8795 - val_loss: 0.3971 - val_accuracy: 0.8151\n",
            "Epoch 80/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2560 - accuracy: 0.8901 - val_loss: 0.4186 - val_accuracy: 0.7983\n",
            "Epoch 81/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2897 - accuracy: 0.8668 - val_loss: 0.3999 - val_accuracy: 0.8403\n",
            "Epoch 82/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.2584 - accuracy: 0.8795 - val_loss: 0.4121 - val_accuracy: 0.8487\n",
            "Epoch 83/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2731 - accuracy: 0.8753 - val_loss: 0.4126 - val_accuracy: 0.8151\n",
            "Epoch 84/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2694 - accuracy: 0.8858 - val_loss: 0.3770 - val_accuracy: 0.8403\n",
            "Epoch 85/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2895 - accuracy: 0.8753 - val_loss: 0.3848 - val_accuracy: 0.8235\n",
            "Epoch 86/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.2469 - accuracy: 0.8901 - val_loss: 0.4070 - val_accuracy: 0.8067\n",
            "Epoch 87/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2542 - accuracy: 0.8837 - val_loss: 0.4007 - val_accuracy: 0.8403\n",
            "Epoch 88/1000\n",
            "4/4 [==============================] - 0s 114ms/step - loss: 0.2706 - accuracy: 0.8647 - val_loss: 0.4152 - val_accuracy: 0.8067\n",
            "Epoch 89/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2890 - accuracy: 0.8605 - val_loss: 0.3937 - val_accuracy: 0.8235\n",
            "Epoch 90/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2670 - accuracy: 0.8774 - val_loss: 0.3946 - val_accuracy: 0.8319\n",
            "Epoch 91/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2912 - accuracy: 0.8605 - val_loss: 0.4292 - val_accuracy: 0.7815\n",
            "Epoch 92/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2544 - accuracy: 0.8837 - val_loss: 0.4064 - val_accuracy: 0.8067\n",
            "Epoch 93/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.2436 - accuracy: 0.8943 - val_loss: 0.4082 - val_accuracy: 0.7899\n",
            "Epoch 94/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2843 - accuracy: 0.8732 - val_loss: 0.4323 - val_accuracy: 0.7731\n",
            "Epoch 95/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2809 - accuracy: 0.8774 - val_loss: 0.3950 - val_accuracy: 0.7899\n",
            "Epoch 96/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2842 - accuracy: 0.8837 - val_loss: 0.3971 - val_accuracy: 0.7563\n",
            "Epoch 97/1000\n",
            "4/4 [==============================] - 0s 106ms/step - loss: 0.2424 - accuracy: 0.8837 - val_loss: 0.4087 - val_accuracy: 0.8319\n",
            "Epoch 98/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2352 - accuracy: 0.8964 - val_loss: 0.4469 - val_accuracy: 0.8067\n",
            "Epoch 99/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2743 - accuracy: 0.8710 - val_loss: 0.4243 - val_accuracy: 0.8151\n",
            "Epoch 100/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2522 - accuracy: 0.8964 - val_loss: 0.4624 - val_accuracy: 0.8151\n",
            "Epoch 101/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2360 - accuracy: 0.9049 - val_loss: 0.4360 - val_accuracy: 0.8235\n",
            "Epoch 102/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2471 - accuracy: 0.8901 - val_loss: 0.4291 - val_accuracy: 0.7983\n",
            "Epoch 103/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2340 - accuracy: 0.8922 - val_loss: 0.4599 - val_accuracy: 0.7899\n",
            "Epoch 104/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2359 - accuracy: 0.8943 - val_loss: 0.4597 - val_accuracy: 0.8151\n",
            "Epoch 105/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2191 - accuracy: 0.9091 - val_loss: 0.5000 - val_accuracy: 0.7647\n",
            "Epoch 106/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2512 - accuracy: 0.8858 - val_loss: 0.4840 - val_accuracy: 0.7983\n",
            "Epoch 107/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2389 - accuracy: 0.8964 - val_loss: 0.4958 - val_accuracy: 0.7647\n",
            "Epoch 108/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2213 - accuracy: 0.8816 - val_loss: 0.4653 - val_accuracy: 0.7899\n",
            "Epoch 109/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2326 - accuracy: 0.8901 - val_loss: 0.4484 - val_accuracy: 0.8067\n",
            "Epoch 110/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2166 - accuracy: 0.8922 - val_loss: 0.4674 - val_accuracy: 0.7983\n",
            "Epoch 111/1000\n",
            "4/4 [==============================] - 0s 114ms/step - loss: 0.2167 - accuracy: 0.9027 - val_loss: 0.4690 - val_accuracy: 0.8403\n",
            "Epoch 112/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2195 - accuracy: 0.9070 - val_loss: 0.4698 - val_accuracy: 0.7899\n",
            "Epoch 113/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.2153 - accuracy: 0.9175 - val_loss: 0.4682 - val_accuracy: 0.7899\n",
            "Epoch 114/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2154 - accuracy: 0.9133 - val_loss: 0.4625 - val_accuracy: 0.7983\n",
            "Epoch 115/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2494 - accuracy: 0.8795 - val_loss: 0.4537 - val_accuracy: 0.8487\n",
            "Epoch 116/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2178 - accuracy: 0.9112 - val_loss: 0.4370 - val_accuracy: 0.8319\n",
            "Epoch 117/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2136 - accuracy: 0.9070 - val_loss: 0.4476 - val_accuracy: 0.7983\n",
            "Epoch 118/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2322 - accuracy: 0.8858 - val_loss: 0.4371 - val_accuracy: 0.8067\n",
            "Epoch 119/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.1986 - accuracy: 0.9070 - val_loss: 0.4497 - val_accuracy: 0.7983\n",
            "Epoch 120/1000\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 0.2276 - accuracy: 0.8964 - val_loss: 0.4818 - val_accuracy: 0.8151\n",
            "Epoch 121/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2440 - accuracy: 0.8964 - val_loss: 0.4960 - val_accuracy: 0.7983\n",
            "Epoch 122/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2275 - accuracy: 0.8943 - val_loss: 0.4603 - val_accuracy: 0.8151\n",
            "Epoch 123/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2543 - accuracy: 0.8837 - val_loss: 0.4089 - val_accuracy: 0.8319\n",
            "Epoch 124/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2490 - accuracy: 0.9027 - val_loss: 0.4036 - val_accuracy: 0.8319\n",
            "Epoch 125/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2859 - accuracy: 0.8605 - val_loss: 0.4148 - val_accuracy: 0.8067\n",
            "Epoch 126/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2483 - accuracy: 0.8943 - val_loss: 0.5067 - val_accuracy: 0.7563\n",
            "Epoch 127/1000\n",
            "4/4 [==============================] - 0s 107ms/step - loss: 0.2634 - accuracy: 0.8901 - val_loss: 0.4560 - val_accuracy: 0.7647\n",
            "Epoch 128/1000\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 0.2639 - accuracy: 0.8732 - val_loss: 0.4589 - val_accuracy: 0.7899\n",
            "Epoch 129/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2676 - accuracy: 0.8943 - val_loss: 0.4688 - val_accuracy: 0.7983\n",
            "Epoch 130/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2257 - accuracy: 0.8901 - val_loss: 0.4649 - val_accuracy: 0.7815\n",
            "Epoch 131/1000\n",
            "4/4 [==============================] - 0s 112ms/step - loss: 0.2580 - accuracy: 0.8753 - val_loss: 0.4751 - val_accuracy: 0.7899\n",
            "Epoch 132/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2433 - accuracy: 0.8837 - val_loss: 0.4975 - val_accuracy: 0.7815\n",
            "Epoch 133/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2574 - accuracy: 0.8816 - val_loss: 0.5087 - val_accuracy: 0.7899\n",
            "Epoch 134/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2410 - accuracy: 0.8964 - val_loss: 0.4906 - val_accuracy: 0.8151\n",
            "Epoch 135/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 0.2321 - accuracy: 0.8943 - val_loss: 0.5137 - val_accuracy: 0.7815\n",
            "Epoch 136/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2677 - accuracy: 0.8710 - val_loss: 0.4952 - val_accuracy: 0.7983\n",
            "Epoch 137/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2570 - accuracy: 0.8816 - val_loss: 0.5089 - val_accuracy: 0.7815\n",
            "Epoch 138/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2382 - accuracy: 0.8901 - val_loss: 0.5013 - val_accuracy: 0.7983\n",
            "Epoch 139/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2318 - accuracy: 0.8964 - val_loss: 0.5233 - val_accuracy: 0.7731\n",
            "Epoch 140/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2566 - accuracy: 0.8837 - val_loss: 0.5096 - val_accuracy: 0.7899\n",
            "Epoch 141/1000\n",
            "4/4 [==============================] - 0s 105ms/step - loss: 0.2301 - accuracy: 0.8964 - val_loss: 0.5030 - val_accuracy: 0.7815\n",
            "Epoch 142/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2315 - accuracy: 0.8922 - val_loss: 0.5136 - val_accuracy: 0.7899\n",
            "Epoch 143/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2405 - accuracy: 0.8858 - val_loss: 0.4897 - val_accuracy: 0.8235\n",
            "Epoch 144/1000\n",
            "4/4 [==============================] - 0s 104ms/step - loss: 0.2095 - accuracy: 0.9175 - val_loss: 0.5562 - val_accuracy: 0.7983\n",
            "Epoch 145/1000\n",
            "4/4 [==============================] - 0s 107ms/step - loss: 0.2284 - accuracy: 0.9218 - val_loss: 0.5337 - val_accuracy: 0.7815\n",
            "Epoch 146/1000\n",
            "4/4 [==============================] - 0s 106ms/step - loss: 0.2020 - accuracy: 0.9070 - val_loss: 0.5274 - val_accuracy: 0.7815\n",
            "Epoch 147/1000\n",
            "4/4 [==============================] - 0s 103ms/step - loss: 0.2273 - accuracy: 0.8816 - val_loss: 0.5406 - val_accuracy: 0.7479\n",
            "Epoch 148/1000\n",
            "4/4 [==============================] - 0s 106ms/step - loss: 0.2233 - accuracy: 0.9049 - val_loss: 0.5454 - val_accuracy: 0.7731\n",
            "Epoch 149/1000\n",
            "4/4 [==============================] - 0s 111ms/step - loss: 0.2286 - accuracy: 0.8922 - val_loss: 0.5378 - val_accuracy: 0.7983\n"
          ]
        }
      ],
      "source": [
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=80, restore_best_weights=True)]\n",
        "\n",
        "history=model.fit(\n",
        "    [x_train1,x_train],\n",
        "    y_train1,\n",
        "    validation_data=([x_test1, x_test], y_test1),\n",
        "    epochs=1000,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Pd9Rqyi5DuRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8db0d4-0971-46fb-c9cd-dd2f4728c887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8403361344537815\n",
            "f1_score 0.7710843373493975\n",
            "jaccard_score 0.6274509803921569\n",
            "roc_auc_score 0.8540829986613119\n",
            "precision_score 0.6808510638297872\n",
            "recall_score 0.8888888888888888\n"
          ]
        }
      ],
      "source": [
        "#testing prediction\n",
        "\n",
        "#testing prediction\n",
        "ypred1=model.predict([x_test1, x_test])\n",
        "\n",
        "ypred=[]\n",
        "#testing prediction\n",
        "for i in ypred1:\n",
        "    if i[0]>i[1]:\n",
        "        ypred.append(0)\n",
        "    else:\n",
        "        ypred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print(accuracy_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(ypred, y_test1))\n",
        "print('roc_auc_score', roc_auc_score(ypred, y_test1))\n",
        "print('precision_score', precision_score(ypred, y_test1))\n",
        "\n",
        "print('recall_score', recall_score(ypred, y_test1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training prediction\n",
        "xpred1=model.predict([x_train1, x_train])\n",
        "xpred=[]\n",
        "\n",
        "for i in xpred1:\n",
        "    if i[0]>i[1]:\n",
        "        xpred.append(0)\n",
        "    else:\n",
        "        xpred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(xpred, y_train1))\n",
        "print('roc_auc_score', roc_auc_score(xpred, y_train1))\n",
        "print('precision_score', precision_score(xpred, y_train1))\n",
        "\n",
        "print('recall_score', recall_score(xpred, y_train1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qdqWUOXQhp4",
        "outputId": "d72153e2-a911-45ac-b4dd-fce2f202c16a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.8942917547568711\n",
            "f1_score 0.8511904761904763\n",
            "jaccard_score 0.7409326424870466\n",
            "roc_auc_score 0.913877338877339\n",
            "precision_score 0.7606382978723404\n",
            "recall_score 0.9662162162162162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d4FMdYRHQiBm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rct_Lottery_features_for_time_series_Machine_Learning (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}