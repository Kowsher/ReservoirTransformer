{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M2icok5AYzvS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/IOT-temp 2.csv')\n",
        "\n",
        "df=df.drop(columns = ['id', 'room_id/id', 'noted_date'])\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "#df = imputer.fit_transform(df)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le1= LabelEncoder()\n",
        "\n",
        "\n",
        "df['out/in']= le1.fit_transform(df['out/in'])\n",
        "\n",
        "'''\n",
        "df=df.dropna()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le1 = LabelEncoder()\n",
        "le2 = LabelEncoder()\n",
        "'''\n",
        "\n",
        "\n",
        "#df['icon']= le.fit_transform(df['icon'])\n",
        "\n",
        "y = df['out/in'].values\n",
        "df=df.drop(columns = ['out/in'])\n",
        "X = df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guBeZhMnFYBm",
        "outputId": "3c74bfd3-2017-42c2-eef8-201318557677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "CgyrLr_T-2Ok",
        "outputId": "834b274e-42df-4db7-eee3-012996241fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(594, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_82uIQCFdziZ",
        "outputId": "71bcf31f-8d2e-4ce3-c12a-d65622ced220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 20345, 1: 77261})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Rn2VoK1owI5"
      },
      "outputs": [],
      "source": [
        "\n",
        "y=y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X=np.concatenate((X[1:], y[0:-1]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxWNccXgu12l",
        "outputId": "0072bc12-e8bb-41a7-ac8b-8b714952eba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=043bc24f494cc7f74c8f5b3c077fc9d8f6c9bde694e68089651281100d8ce73e\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8iIZ1105tFnf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X= sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl0SFJ5WtbBP",
        "outputId": "d24a352b-0307-4dd9-8689-df2e808e5d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyrcn\n",
            "  Downloading PyRCN-0.0.16.post1-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 51 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81 kB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyrcn\n",
            "Successfully installed pyrcn-0.0.16.post1\n"
          ]
        }
      ],
      "source": [
        "pip install pyrcn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gEPWGptbuTYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y[1:], test_size=0.2, shuffle=False, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDAxxhLupYK6",
        "outputId": "3c343778-2705-4f33-89b7-e9047657317d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78084, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_train1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1rOeQ0-ptl_G"
      },
      "outputs": [],
      "source": [
        "from pyrcn.base.blocks import InputToNode\n",
        "from sklearn . datasets import make_blobs\n",
        "# Generate a toy dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tShY5ktnrN",
        "outputId": "3f848e1a-0b0d-43a4-d311-9f1eab79942c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=8 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "input_to_node = InputToNode (8, input_activation='relu',input_scaling =1.0 )\n",
        "\n",
        "\n",
        "x_train= input_to_node.fit_transform (x_train1)\n",
        "x_test= input_to_node.transform (x_test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDw3vyTRuB-o",
        "outputId": "e5552625-9372-4ee4-f3dd-59dafad87258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:77: FutureWarning: Pass hidden_layer_size=8 as keyword args. From version 1.1 (renaming of 0.26) passing these as positional arguments will result in an error\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from pyrcn.base.blocks import NodeToNode\n",
        "node_to_node = NodeToNode (8, reservoir_activation='relu', spectral_radius =1.0 , leakage =0.8 ,bidirectional = False )\n",
        "x_train=node_to_node . fit_transform(x_train)\n",
        "x_test= node_to_node.transform (x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b-dqEbXtuCpY"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "x_train1 = x_train1.reshape((x_train1.shape[0], x_train1.shape[1], 1))\n",
        "x_test1 = x_test1.reshape((x_test1.shape[0], x_test1.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qi0MhVmsuswU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NX3w9IqBuuZ-"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HsU6bO-PuvqD"
      },
      "outputs": [],
      "source": [
        "from keras_self_attention import SeqSelfAttention\n",
        "def build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "):\n",
        "    inputsy = keras.Input(shape=input_shapey)\n",
        "    inputsz = keras.Input(shape=input_shapez)\n",
        "    y = inputsy\n",
        "    z= inputsz\n",
        "\n",
        "    #z=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(z)\n",
        "\n",
        "    #y=layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(y)\n",
        "\n",
        "    z=SeqSelfAttention()(z)\n",
        "\n",
        "    #y=SeqSelfAttention()(y)\n",
        "\n",
        "\n",
        "\n",
        "    x=layers.Concatenate(axis=1)([y, z])\n",
        "    #x=layers.Add()([inputsy, inputsz])\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "    #outputs = layers.Dense(1)(x)\n",
        "    return keras.Model([inputsy,inputsz], outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Hu3CH4vZzu",
        "outputId": "2ef43194-0e66-4487-ce5c-6d1a12dfa8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 8, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 2, 1)]       0           []                               \n",
            "                                                                                                  \n",
            " seq_self_attention (SeqSelfAtt  (None, 8, 1)        129         ['input_2[0][0]']                \n",
            " ention)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 10, 1)        0           ['input_1[0][0]',                \n",
            "                                                                  'seq_self_attention[0][0]']     \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 10, 1)       2           ['concatenate[0][0]']            \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 10, 1)       7169        ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 10, 1)        0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 10, 1)       0           ['dropout[0][0]',                \n",
            " da)                                                              'concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 10, 4)        8           ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 10, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 10, 1)        5           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 10, 1)       0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 10, 1)       7169        ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 10, 1)       0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 10, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 10, 1)        5           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 10, 1)       0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 10, 1)       7169        ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 10, 1)       0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 10, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 10, 1)        5           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 10, 1)       0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 10, 1)       7169        ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 10, 1)        0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 10, 1)       0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 10, 1)       2           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 10, 4)        8           ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 10, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 10, 1)        5           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 10, 1)       0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 10)          0           ['tf.__operators__.add_7[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          1408        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 2)            258         ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 30,539\n",
            "Trainable params: 30,539\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shapey = x_train1.shape[1:]\n",
        "input_shapez = x_train.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shapey,\n",
        "    input_shapez,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"mean_absolute_error\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"mean_absolute_error\"],\n",
        ")\n",
        "'''\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jta9P_Q3vkrc",
        "outputId": "b757b204-4816-4ca0-ee03-3e015637d46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "611/611 [==============================] - 23s 25ms/step - loss: 0.1978 - sparse_categorical_accuracy: 0.9039 - val_loss: 0.4835 - val_sparse_categorical_accuracy: 0.7326\n",
            "Epoch 2/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1617 - sparse_categorical_accuracy: 0.9203 - val_loss: 0.4908 - val_sparse_categorical_accuracy: 0.7356\n",
            "Epoch 3/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1640 - sparse_categorical_accuracy: 0.9187 - val_loss: 0.4590 - val_sparse_categorical_accuracy: 0.7612\n",
            "Epoch 4/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1621 - sparse_categorical_accuracy: 0.9187 - val_loss: 0.4693 - val_sparse_categorical_accuracy: 0.7401\n",
            "Epoch 5/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1617 - sparse_categorical_accuracy: 0.9186 - val_loss: 0.4653 - val_sparse_categorical_accuracy: 0.7699\n",
            "Epoch 6/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1663 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.4677 - val_sparse_categorical_accuracy: 0.7291\n",
            "Epoch 7/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9200 - val_loss: 0.5452 - val_sparse_categorical_accuracy: 0.7340\n",
            "Epoch 8/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1591 - sparse_categorical_accuracy: 0.9197 - val_loss: 0.4715 - val_sparse_categorical_accuracy: 0.7395\n",
            "Epoch 9/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1570 - sparse_categorical_accuracy: 0.9214 - val_loss: 0.4736 - val_sparse_categorical_accuracy: 0.7442\n",
            "Epoch 10/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1616 - sparse_categorical_accuracy: 0.9205 - val_loss: 0.4607 - val_sparse_categorical_accuracy: 0.7521\n",
            "Epoch 11/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1509 - sparse_categorical_accuracy: 0.9239 - val_loss: 0.4896 - val_sparse_categorical_accuracy: 0.7298\n",
            "Epoch 12/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1524 - sparse_categorical_accuracy: 0.9251 - val_loss: 0.4760 - val_sparse_categorical_accuracy: 0.7614\n",
            "Epoch 13/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1572 - sparse_categorical_accuracy: 0.9225 - val_loss: 0.4577 - val_sparse_categorical_accuracy: 0.7423\n",
            "Epoch 14/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9259 - val_loss: 0.4571 - val_sparse_categorical_accuracy: 0.7555\n",
            "Epoch 15/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1521 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.4737 - val_sparse_categorical_accuracy: 0.7646\n",
            "Epoch 16/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1543 - sparse_categorical_accuracy: 0.9234 - val_loss: 0.4602 - val_sparse_categorical_accuracy: 0.7399\n",
            "Epoch 17/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.2519 - sparse_categorical_accuracy: 0.9066 - val_loss: 0.4743 - val_sparse_categorical_accuracy: 0.7285\n",
            "Epoch 18/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1650 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.4624 - val_sparse_categorical_accuracy: 0.7433\n",
            "Epoch 19/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1561 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.4605 - val_sparse_categorical_accuracy: 0.7554\n",
            "Epoch 20/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1510 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.4546 - val_sparse_categorical_accuracy: 0.7512\n",
            "Epoch 21/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1477 - sparse_categorical_accuracy: 0.9266 - val_loss: 0.4676 - val_sparse_categorical_accuracy: 0.7585\n",
            "Epoch 22/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1470 - sparse_categorical_accuracy: 0.9267 - val_loss: 0.4591 - val_sparse_categorical_accuracy: 0.7355\n",
            "Epoch 23/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1469 - sparse_categorical_accuracy: 0.9268 - val_loss: 0.4839 - val_sparse_categorical_accuracy: 0.7378\n",
            "Epoch 24/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1496 - sparse_categorical_accuracy: 0.9259 - val_loss: 0.4624 - val_sparse_categorical_accuracy: 0.7576\n",
            "Epoch 25/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1490 - sparse_categorical_accuracy: 0.9262 - val_loss: 0.4539 - val_sparse_categorical_accuracy: 0.7737\n",
            "Epoch 26/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1598 - sparse_categorical_accuracy: 0.9211 - val_loss: 0.4657 - val_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 27/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9261 - val_loss: 0.4581 - val_sparse_categorical_accuracy: 0.7711\n",
            "Epoch 28/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1500 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.4606 - val_sparse_categorical_accuracy: 0.7622\n",
            "Epoch 29/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1588 - sparse_categorical_accuracy: 0.9225 - val_loss: 0.4544 - val_sparse_categorical_accuracy: 0.7722\n",
            "Epoch 30/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9255 - val_loss: 0.4654 - val_sparse_categorical_accuracy: 0.7722\n",
            "Epoch 31/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1691 - sparse_categorical_accuracy: 0.9217 - val_loss: 0.4799 - val_sparse_categorical_accuracy: 0.7350\n",
            "Epoch 32/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1457 - sparse_categorical_accuracy: 0.9278 - val_loss: 0.4547 - val_sparse_categorical_accuracy: 0.7686\n",
            "Epoch 33/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1485 - sparse_categorical_accuracy: 0.9259 - val_loss: 0.4857 - val_sparse_categorical_accuracy: 0.7384\n",
            "Epoch 34/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1661 - sparse_categorical_accuracy: 0.9208 - val_loss: 0.4511 - val_sparse_categorical_accuracy: 0.7680\n",
            "Epoch 35/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1435 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.4598 - val_sparse_categorical_accuracy: 0.7441\n",
            "Epoch 36/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1452 - sparse_categorical_accuracy: 0.9283 - val_loss: 0.4650 - val_sparse_categorical_accuracy: 0.7515\n",
            "Epoch 37/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1573 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.4657 - val_sparse_categorical_accuracy: 0.7634\n",
            "Epoch 38/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1449 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.4526 - val_sparse_categorical_accuracy: 0.7716\n",
            "Epoch 39/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1600 - sparse_categorical_accuracy: 0.9242 - val_loss: 0.6726 - val_sparse_categorical_accuracy: 0.6251\n",
            "Epoch 40/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.2192 - sparse_categorical_accuracy: 0.9029 - val_loss: 0.4630 - val_sparse_categorical_accuracy: 0.7392\n",
            "Epoch 41/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1485 - sparse_categorical_accuracy: 0.9255 - val_loss: 0.4536 - val_sparse_categorical_accuracy: 0.7688\n",
            "Epoch 42/100\n",
            "611/611 [==============================] - 15s 24ms/step - loss: 0.1459 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.4712 - val_sparse_categorical_accuracy: 0.7589\n",
            "Epoch 43/100\n",
            "611/611 [==============================] - 14s 24ms/step - loss: 0.1433 - sparse_categorical_accuracy: 0.9284 - val_loss: 0.4597 - val_sparse_categorical_accuracy: 0.7650\n",
            "Epoch 44/100\n",
            "611/611 [==============================] - 14s 23ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.9263 - val_loss: 0.4672 - val_sparse_categorical_accuracy: 0.7567\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "history=model.fit(\n",
        "    [x_train1,x_train],\n",
        "    y_train1,\n",
        "    validation_data=([x_test1, x_test], y_test1),\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42doJdzh7mKo",
        "outputId": "74544702-7a5d-46c4-dcda-b2a96402f1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7680446698427335\n",
            "f1_score 0.8338104675915731\n",
            "jaccard_score 0.7149870963680997\n",
            "roc_auc_score 0.8001317156380435\n",
            "precision_score 0.9492729399966572\n",
            "recall_score 0.743390052356021\n"
          ]
        }
      ],
      "source": [
        "#testing prediction\n",
        "\n",
        "#testing prediction\n",
        "ypred1=model.predict([x_test1, x_test])\n",
        "\n",
        "ypred=[]\n",
        "#testing prediction\n",
        "for i in ypred1:\n",
        "    if i[0]>i[1]:\n",
        "        ypred.append(0)\n",
        "    else:\n",
        "        ypred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print(accuracy_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(ypred, y_test1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(ypred, y_test1))\n",
        "print('roc_auc_score', roc_auc_score(ypred, y_test1))\n",
        "print('precision_score', precision_score(ypred, y_test1))\n",
        "\n",
        "print('recall_score', recall_score(ypred, y_test1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9Rqyi5DuRJ",
        "outputId": "a89d3675-4e42-45c3-adf0-d6614b8c82c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9297807489370422\n",
            "f1_score 0.9593842826136877\n",
            "jaccard_score 0.9219390660592255\n",
            "roc_auc_score 0.9324349808481128\n",
            "precision_score 0.9917604717053373\n",
            "recall_score 0.9290551203695733\n"
          ]
        }
      ],
      "source": [
        "#training prediction\n",
        "xpred1=model.predict([x_train1, x_train])\n",
        "xpred=[]\n",
        "\n",
        "for i in xpred1:\n",
        "    if i[0]>i[1]:\n",
        "        xpred.append(0)\n",
        "    else:\n",
        "        xpred.append(1)\n",
        "from sklearn.metrics import accuracy_score, f1_score,  jaccard_score, roc_auc_score, precision_score, recall_score\n",
        "print('accuracy', accuracy_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('f1_score', f1_score(xpred, y_train1))\n",
        "\n",
        "\n",
        "\n",
        "print('jaccard_score', jaccard_score(xpred, y_train1))\n",
        "print('roc_auc_score', roc_auc_score(xpred, y_train1))\n",
        "print('precision_score', precision_score(xpred, y_train1))\n",
        "\n",
        "print('recall_score', recall_score(xpred, y_train1))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rct-Temperature Readings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}